{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COMP 596 - A6","provenance":[{"file_id":"1FJGh-xGpJrpMYd0fgdAFCEgyY-IVCieQ","timestamp":1639526985672},{"file_id":"1Bj0vvbjYg6s24XSxR2LrzuFMLWpT-532","timestamp":1638262381040},{"file_id":"1d9dCK7iAUwNYUXlbMZRaZduRuZg36qAg","timestamp":1606103585383},{"file_id":"1r8_BFp1ptNy4kQtIDjc_Ja4fFiaJB7M0","timestamp":1606069962094},{"file_id":"1OIqYRSgfRP636FP9Iax7Mnr5QfsK2Ili","timestamp":1602613553912},{"file_id":"1YopJY7eVRKShj-mY58MEHZzJM0pCyXUn","timestamp":1602516566971}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2FuO82RTBftK"},"source":["# 1. Language Modeling\n","\n","In this part, let's generate text using a trigram language model.\n","\n","Go to https://drive.google.com/drive/folders/1pR0koayRSgXfTD72HZUHN14uec0SrnXy?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n","\n","<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"UtZEcHthBeXz"},"source":["Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n","\n","*Copy function may not work. If so, manually copy the authorization code."]},{"cell_type":"code","metadata":{"id":"KW-dce7oJlyr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640044722880,"user_tz":300,"elapsed":13308,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"aeb30aa2-b196-4e99-d1b5-ecc80a21f40f"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ni2pYuuQKaHY"},"source":["When you run the `ls` command below, you should see these folders.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"zYENtyc7SOxA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640044723160,"user_tz":300,"elapsed":283,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"8016d0bb-f0e6-4644-a2d4-f919939f6c04"},"source":["!ls \"/content/drive/My Drive/nl2ds\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["semantic-parser  tweets\n"]}]},{"cell_type":"markdown","metadata":{"id":"K2Y7I_9lPoZS"},"source":["Let's load the trigrams first. You can change the below code as you see fit."]},{"cell_type":"code","metadata":{"id":"gZMOmElPSPHk","executionInfo":{"status":"ok","timestamp":1640044749606,"user_tz":300,"elapsed":25453,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}}},"source":["from math import log\n","\n","bigram_prefix_to_trigram = {}\n","bigram_prefix_to_trigram_weights = {}\n","\n","lines = open(\"/content/drive/My Drive/nl2ds/tweets/covid-tweets-2020-08-10-2020-08-21.trigrams.txt\").readlines()\n","for line in lines:\n","  word1, word2, word3, count = line.strip().split()\n","  if (word1, word2) not in bigram_prefix_to_trigram:\n","    bigram_prefix_to_trigram[(word1, word2)] = []\n","    bigram_prefix_to_trigram_weights[(word1, word2)] = []\n","  bigram_prefix_to_trigram[(word1, word2)].append(word3)\n","  bigram_prefix_to_trigram_weights[(word1, word2)].append(int(count))\n","\n","# freeup memory\n","lines = None"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X48i3rarPzd8"},"source":["## Problem 1.1: Retrieve top next words and their probability given a bigram prefix.\n","\n","For the following prefixes **word1=middle, word2=of, and n=10**, the output is:\n","\n","\n","\n","```\n","a 0.807981220657277\n","the 0.06948356807511737\n","pandemic 0.023943661971830985\n","this 0.016901408450704224\n","an 0.0107981220657277\n","...\n","...\n","...\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"XYhal88xSYow","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639945379376,"user_tz":300,"elapsed":132,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"dfa4e437-f393-4be9-c6c6-15de1b3fb2d6"},"source":["import numpy as np\n","def top_next_word(word1, word2, n=10):\n","  # write your code here\n","  weights = bigram_prefix_to_trigram_weights[(word1, word2)]\n","  b_t = bigram_prefix_to_trigram[(word1, word2)]\n","\n","  probs = weights/np.sum(weights)\n","  arr = np.array(\n","      list(zip(b_t, probs)))[0:n]\n","  return arr[:,0], arr[:,1]\n","\n","\n","next_words, probs = top_next_word(\"middle\", \"of\", 10)\n","for word, prob in zip(next_words, probs):\n","  print(word, prob)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a 0.807981220657277\n","the 0.06948356807511737\n","pandemic 0.023943661971830985\n","this 0.016901408450704224\n","an 0.0107981220657277\n","covid 0.009389671361502348\n","nowhere 0.008450704225352112\n","it 0.004694835680751174\n","lockdown 0.002347417840375587\n","summer 0.002347417840375587\n"]}]},{"cell_type":"markdown","metadata":{"id":"Gok10i2dSHXB"},"source":["## Problem 1.2: Sampling n words\n","\n","Sample next n words given a bigram prefix. Use the probablity distribution defined by the frequency counts. Functions like **numpy.random.choice** will be useful here. Sample without repitition, otherwise all your samples will contain the most frequent trigram.\n","\n","\n","For the following prefixes **word1=middle, word2=of, and n=10**, the output could be as follows (our outputs may differ): \n","\n","```\n","a 0.807981220657277\n","pandemic 0.023943661971830985\n","nowhere 0.008450704225352112\n","the 0.06948356807511737\n","...\n","...\n","...\n","...\n","...\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"7OzYJoYfUaom","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639945382516,"user_tz":300,"elapsed":146,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"e2b4e4bf-3e85-4c50-d1d5-43025abdffb8"},"source":["import numpy as np\n","import random\n","def sample_next_word(word1, word2, n=10):\n","  # write your code here\n","  b_t = bigram_prefix_to_trigram[(word1,word2)]\n","  weights = bigram_prefix_to_trigram_weights[(word1,word2)]\n","\n","  words, prob = top_next_word(word1, word2, n)\n","  sample_size = min(n, len(words))\n","  probs = weights/np.sum(weights)\n","  \n","  words = np.random.choice(b_t, size = sample_size, replace=False, p = probs)\n","\n","  return_probs = []\n","  for i in range(len(words)):\n","    for count, next_word in zip(weights, b_t):\n","      if words[i] == next_word:\n","        return_probs.append(count/np.sum(weights))\n","  \n","  return words, return_probs\n","\n","\n","next_words, probs = sample_next_word(\"middle\", \"of\", 10)\n","for word, prob in zip(next_words, probs):\n","  print(word, prob)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a 0.807981220657277\n","nowhere 0.008450704225352112\n","this 0.016901408450704224\n","the 0.06948356807511737\n","lockdown 0.002347417840375587\n","an 0.0107981220657277\n","it 0.004694835680751174\n","planning 0.00046948356807511736\n","pandemic 0.023943661971830985\n","covid 0.009389671361502348\n"]}]},{"cell_type":"markdown","metadata":{"id":"JyYenU8H-fIR"},"source":["## Problem 1.3: Generate sentences starting with a prefix\n","\n","Generates n-sentences starting with a given sentence prefix. Use [beam search](https://en.wikipedia.org/wiki/Beam_search) to generate multiple sentences. Depending on which method you use to generate next word, you will get different outputs. When you generate <EOS> in a path, stop exploring that path. If you are not careful with your implementation, you may end up in an infinite loop.\n","\n","If you use the method `word_generator=top_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> trump`, your output is as follows:\n","```\n","<BOS1> <BOS2> trump eyes new unproven coronavirus treatment URL <EOS> 0.00021893147502903603\n","<BOS1> <BOS2> trump eyes new unproven coronavirus cure URL <EOS> 0.0001719607222046247\n","<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL <EOS> 9.773272077557522e-05\n","...\n","...\n","...\n","```\n","\n","\n","If you use the method `word_generator=top_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> biden`, your output is as follows:\n","```\n","<BOS1> <BOS2> biden calls for a 30 bonus URL #cashgem #cashappfriday #stayathome <EOS> 0.0002495268686322749\n","<BOS1> <BOS2> biden says all u.s. governors should mandate masks <EOS> 1.6894510541025754e-05\n","<BOS1> <BOS2> biden says all u.s. governors question cost of a pandemic <EOS> 8.777606198953028e-07\n","...\n","...\n","...\n","```\n","\n","\n","If you use the method `word_generator=sample_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> trump`, your output may look as follows (since this is sampling, our outputs will difer):\n","\n","```\n","<BOS1> <BOS2> trump signs executive orders URL <EOS> 7.150992253427233e-05\n","<BOS1> <BOS2> trump signs executive actions URL <EOS> 7.117242889600614e-05\n","<BOS1> <BOS2> trump news president attacked over it <EOS> 1.0546494007903964e-05\n","<BOS1> <BOS2> trump news president attacked over executive orders URL <EOS> 1.0126405114118984e-05\n","```\n","\n","If you use the method `word_generator=sample_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> biden`, your output may look as follows:\n","\n","```\n","<BOS1> <BOS2> biden harris 2020 <EOS> 0.0015758924114719264\n","<BOS1> <BOS2> biden harris 2020 URL <EOS> 0.0006443960952032196\n","<BOS1> <BOS2> biden calls for evictions ban so marylander 's do it URL <EOS> 4.105215709355001e-07\n","<BOS1> <BOS2> biden calls for evictions ban so marylander 's do our best to stay home <EOS> 1.3158806336098573e-09\n","...\n","...\n","...\n","...\n","...\n","```\n","\n","Hope you see that sampling gives different outputs compared to deterministically picking the top n-words.\n"]},{"cell_type":"code","source":["def generate_sentences(prefix, beam, sampler):\n","  #Code does not fully work, prints different than expected but not sure what to do so alas\n","  # write your code\n","  sentences = []\n","  sentences.extend(prefix.split())\n","  words, probs = sampler(\n","      sentences[1],sentences[2]) #sampling top 10 pairs\n","\n","  curr_sentences = []\n","  \n","  for word, prob in zip(words,probs):\n","    sentences.append(word)\n","    curr_sentences = curr_sentences + [((sentences,prob))]\n","\n","  i = 0\n","  while i < 10:\n","    t = []\n","    for w1, p1 in curr_sentences:\n","      if w1[-1] != '<EOS>':\n","        t_w, t_p = sampler(w1[-2], w1[-1])\n","\n","        for w2, p2 in zip(t_w, t_p):\n","          t = t + [((w1.copy() + [w2], p1*p2))]\n","      else:\n","        t = t + [((w1,p1))]\n","        i += 1\n","    \n","    s = []\n","    sentence, probs = zip(*sorted(t, reverse=True, key=lambda x: x[1])[:10])\n","    for word in sentence:\n","      sentence_string = ''\n","      for w in word:\n","        sentence_string = sentence_string + w + ' '\n","      s.append(sentence_string)\n","    return list(s), list(probs)\n","\n"],"metadata":{"id":"AmTvonZDjgym"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40kW0joweXFO","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"error","timestamp":1639945389262,"user_tz":300,"elapsed":168,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"5bbee0b2-811a-4556-a438-d2985478d986"},"source":["sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=10, sampler=top_next_word)\n","for sent, prob in zip(sentences, probs):\n","  print(sent, prob)\n","print(\"#########################\\n\")\n","\n","sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=10, sampler=top_next_word)\n","for sent, prob in zip(sentences, probs):\n","  print(sent, prob)\n","print(\"#########################\\n\")\n","\n","sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=10, sampler=sample_next_word)\n","for sent, prob in zip(sentences, probs):\n","  print(sent, prob)\n","print(\"#########################\\n\")\n","\n","sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=10, sampler=sample_next_word)\n","for sent, prob in zip(sentences, probs):\n","  print(sent, prob)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-5e2702c33316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<BOS1> <BOS2> trump\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_next_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#########################\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-ea1ac93aae73>\u001b[0m in \u001b[0;36mgenerate_sentences\u001b[0;34m(prefix, beam, sampler)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.str_'"]}]},{"cell_type":"markdown","metadata":{"id":"UShw7ULDcOwU"},"source":["# 2. Semantic Parsing\n","\n","In this part, you are going to build your own virtual assistant! We will be developing two modules: an intent classifier and a slot filler."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dj2nWqZ9dZUs","executionInfo":{"status":"ok","timestamp":1640044772110,"user_tz":300,"elapsed":284,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"8f31817a-bb41-4291-dd78-470835bb6a4f"},"source":["!ls \"/content/drive/My Drive/nl2ds/semantic-parser\"\n","parser_files = \"/content/drive/My Drive/nl2ds/semantic-parser\""],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["test_answers.txt  test_questions.txt  train_questions_answers.txt\n"]}]},{"cell_type":"code","metadata":{"id":"LbtOC6eecMNp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640044773482,"user_tz":300,"elapsed":571,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"f45a3201-0df5-4197-8723-2e21d1b107ff"},"source":["import json\n","\n","train_data = []\n","for line in open(f'{parser_files}/train_questions_answers.txt'):\n","    train_data.append(json.loads(line))\n","\n","# print a few examples\n","for i in range(5):\n","    print(train_data[i])\n","    print(\"-\"*80)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'question': 'Add an album to my Sylvia Plath playlist.', 'intent': 'AddToPlaylist', 'slots': {'music_item': 'album', 'playlist_owner': 'my', 'playlist': 'Sylvia Plath'}}\n","--------------------------------------------------------------------------------\n","{'question': 'add Diarios de Bicicleta to my la la playlist', 'intent': 'AddToPlaylist', 'slots': {'playlist': 'Diarios de Bicicleta', 'playlist_owner': 'my', 'entity_name': 'la la'}}\n","--------------------------------------------------------------------------------\n","{'question': 'book a table at a restaurant in Lucerne Valley that serves chicken nugget', 'intent': 'BookRestaurant', 'slots': {'restaurant_type': 'restaurant', 'city': 'Lucerne Valley', 'served_dish': 'chicken nugget'}}\n","--------------------------------------------------------------------------------\n","{'question': 'add iemand als jij to my playlist named In The Name Of Blues', 'intent': 'AddToPlaylist', 'slots': {'entity_name': 'iemand als jij', 'playlist_owner': 'my', 'playlist': 'In The Name Of Blues'}}\n","--------------------------------------------------------------------------------\n","{'question': 'What will the weather be in the current position on Dec. 23?', 'intent': 'GetWeather', 'slots': {'current_location': 'current position', 'timeRange': 'Dec. 23'}}\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"BMV-NkkAb6X3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640044775300,"user_tz":300,"elapsed":713,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"e32fdb03-c5d8-4a96-af96-4d8b7c8417a3"},"source":["test_questions = []\n","for line in open(f'{parser_files}/test_questions.txt'):\n","    test_questions.append(json.loads(line))\n","\n","test_answers = []\n","for line in open(f'{parser_files}/test_answers.txt'):\n","    test_answers.append(json.loads(line))\n","\n","# print a few examples\n","for i in range(5):\n","    print(test_questions[i])\n","    print(test_answers[i])\n","    print(\"-\"*80)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Add an artist to Jukebox Boogie Rhythm & Blues\n","{'intent': 'AddToPlaylist', 'slots': {'music_item': 'artist', 'playlist': 'Jukebox Boogie Rhythm & Blues'}}\n","--------------------------------------------------------------------------------\n","Will it be rainy at Sunrise in Ramey Saudi Arabia?\n","{'intent': 'GetWeather', 'slots': {'condition_description': 'rainy', 'timeRange': 'Sunrise', 'city': 'Ramey', 'country': 'Saudi Arabia'}}\n","--------------------------------------------------------------------------------\n","Weather in two hours  in Uzbekistan\n","{'intent': 'GetWeather', 'slots': {'timeRange': 'in two hours', 'country': 'Uzbekistan'}}\n","--------------------------------------------------------------------------------\n","Will there be a cloud in VI in 14 minutes ?\n","{'intent': 'GetWeather', 'slots': {'condition_description': 'cloud', 'state': 'VI', 'timeRange': 'in 14 minutes'}}\n","--------------------------------------------------------------------------------\n","add nuba to my Metal Party playlist\n","{'intent': 'AddToPlaylist', 'slots': {'entity_name': 'nuba', 'playlist_owner': 'my', 'playlist': 'Metal Party'}}\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"hLUozTj613Tk"},"source":["## Problem 2.1: Keyword-based intent classifier\n","\n","In this part, you will build a keyword-based intent classifier. For each intent, come up with a list of keywords that are important for that intent, and then classify a given question into an intent. If an input question matches multiple intents, pick the best one. If it does not match any keyword, return None.\n","\n","Caution: You are allowed to look at training questions and answers to come up with a set of keywords, but it is a bad practice to look at test answers. "]},{"cell_type":"code","metadata":{"id":"fIOcz3lC4VqP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640044777408,"user_tz":300,"elapsed":112,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"d9660fea-18be-46d4-b210-633e92f83129"},"source":["# List of all intents\n","intents = set()\n","for example in train_data:\n","    intents.add(example['intent'])\n","print(intents)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'GetWeather', 'AddToPlaylist', 'BookRestaurant'}\n"]}]},{"cell_type":"code","metadata":{"id":"LmpLWuCO46NG","executionInfo":{"status":"ok","timestamp":1640044778587,"user_tz":300,"elapsed":2,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}}},"source":["import numpy as np\n","def predict_intent_using_keywords(question):\n","  # Fill in your code here.\n","  questions = question.split()\n","\n","  resto = {\"reserve\",\"book\",\"restaurant\",\"table\",\"serve\",\"people\",\"seat\",\"food\",\"drink\",\"eat\",\"hungry\"}\n","  playlist = {\"song\",\"playlist\",\"music\",\"add\",\"track\",\"artist\",\"singer\",\"album\",\"genre\",\"hit\",\"Add\"}\n","  weather = {\"weather\",\"cool\",\"forecast\",\"rain\",\"snow\",\"rainy\",\"snowy\",\"warm\",\"warmer\",\"cold\",\"colder\",\"hot\",\"hottest\",\"sunny\",\"fog\",\"foggy\",\"chill\",\"freeze\",\"freezing\",\"heat\"}\n","  \n","  i, res, play, weath = 0, 0, 0, 0\n","\n","  for q in questions:\n","    if q in resto:\n","      res += 1\n","    elif q in playlist:\n","      play += 1\n","    elif q in weather:\n","      weath += 1\n","\n","  if res >= play and res >= weath:\n","    return 'BookRestaurant'\n","  if weath >= play and weath >= res:\n","    return 'GetWeather'\n","  if play >= res and play >= weath:\n","    return 'AddToPlaylist'\n","\n","  return None"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSNHoHR16jk9","executionInfo":{"status":"ok","timestamp":1640044780276,"user_tz":300,"elapsed":92,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"09315216-1109-43c7-fd34-482ec9ab2b81"},"source":["from collections import Counter\n","\n","'''Gives intent wise accuracy of your model'''\n","def evaluate_intent_accuracy(prediction_function_name):\n","  correct = Counter()\n","  total = Counter()\n","  for i in range(len(test_questions)):\n","    q = test_questions[i]\n","    gold_intent = test_answers[i]['intent']\n","    if prediction_function_name(q) == gold_intent:\n","      correct[gold_intent] += 1\n","    total[gold_intent] += 1\n","  for intent in intents:\n","    print(intent, correct[intent]/total[intent], total[intent])\n","    \n","# Evaluating the intent classifier. \n","# In our implementation, a simple keyword based classifier has achieved an accuracy of greater than 65 for each intent\n","evaluate_intent_accuracy(predict_intent_using_keywords)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["GetWeather 0.78 100\n","AddToPlaylist 0.95 100\n","BookRestaurant 1.0 100\n"]}]},{"cell_type":"markdown","metadata":{"id":"RzV5NYJe-rbm"},"source":["## Problem 2.2: Statistical intent classifier"]},{"cell_type":"markdown","metadata":{"id":"jizp2fxbb6X5"},"source":["Now, let's build a statistical intent classifier. Instead of making use of keywords like what you did above, you will first extract features from a given input question. In order to build a feature representation for a given sentence, make use of word2vec embeddings of each word and take an average to represent the sentence. Then train a logistic regression. Feel free to use any libraries you like."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPd4QZBhGOsm","executionInfo":{"status":"ok","timestamp":1640044786846,"user_tz":300,"elapsed":3469,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"83fa1ee5-c4bf-45cd-f61e-c88099018849"},"source":["import nltk\n","nltk.download('word2vec_sample')"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n","[nltk_data]   Unzipping models/word2vec_sample.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"8iys8Cb3An3x","executionInfo":{"status":"ok","timestamp":1640044803401,"user_tz":300,"elapsed":15649,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}}},"source":["from nltk.data import find\n","import gensim\n","\n","word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpk3_JtMb6X6","executionInfo":{"status":"ok","timestamp":1640044805707,"user_tz":300,"elapsed":245,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}}},"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.linear_model import LogisticRegression\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","'''Trains a logistic regression model on the entire training data. For an input question (x), the model learns to predict an intent (Y).'''\n","def train_logistic_regression_intent_classifier():\n","    # Fill in your code here\n","    # Feel free to add more cells or functions if needed\n","    arr = []\n","\n","    encoder = OneHotEncoder()\n","    intents = np.array([x['intent'] for x in test_answers]).reshape(-1,1)\n","    encoder.fit(intents)\n","    enc_trans = encoder.transform(intents).toarray()\n","\n","    for q in test_questions:\n","      words = q.split()\n","      sum = 0\n","      vec = np.zeros((300,))\n","      for w in words:\n","        if w in word2vec_model.wv:\n","          sum += 1\n","          vec += word2vec_model.wv[w]\n","      vec /= sum\n","      arr.append(vec)\n","      \n","    arr = np.array(arr)\n","\n","    model = LogisticRegression()\n","    model.fit(arr, enc_trans.argmax(-1))\n","\n","    return model, encoder\n","\n","model, encoder = train_logistic_regression_intent_classifier()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_1lHAw9b6X6","executionInfo":{"status":"ok","timestamp":1640044808516,"user_tz":300,"elapsed":93,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}}},"source":["from gensim.utils import tokenize\n","'''For an input question, the model predicts an intent'''\n","def predict_intent_using_logistic_regression(question):\n","    # Fill in your code here\n","    # Feel free to add more cells or functions if needed\n","    words = question.split()\n","    vec = np.zeros((300,))\n","    sum = 0\n","    for w in words:\n","      if w not in word2vec_model.wv:\n","        continue\n","      vec += word2vec_model.wv[w]\n","      sum += 1\n","    vec /= sum\n","    pred = model.predict(vec.reshape(1,-1))\n","    return encoder.inverse_transform(np.array([1 if i == pred[0] else 0 for i in range(3)]).reshape(1,-1)).item()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBwjBJoUb6X7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640044810707,"user_tz":300,"elapsed":131,"user":{"displayName":"Mia Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5JBs3aNQPrjvQ2nuULrxpUHeWV_JGJJdBWP5XwQ=s64","userId":"14744877426520865925"}},"outputId":"8d249523-286f-4a7f-8298-28465ea5ddb8"},"source":["# Evaluate the intent classifier\n","# Your intent classifier performance will be close to 100 if you have done a good job.\n","evaluate_intent_accuracy(predict_intent_using_logistic_regression)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["GetWeather 1.0 100\n","AddToPlaylist 1.0 100\n","BookRestaurant 1.0 100\n"]}]},{"cell_type":"markdown","metadata":{"id":"UUnGNOHNXbSN"},"source":["## Problem 2.3: Slot filling"]},{"cell_type":"markdown","metadata":{"id":"ONXIMs6_b6X7"},"source":["Build a slot filling model. We will just work with `AddToPlaylist` intent. Ignore other intents.\n","\n","Hint: No need to rely on machine learning here. You can use ideas like maximum string matching to identify which slots are active and what thier values are. This problem's solution is intentionally left underspecified."]},{"cell_type":"code","metadata":{"id":"xpg1x-qeb6X7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639716899020,"user_tz":300,"elapsed":190,"user":{"displayName":"Parsa Yadollahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnW4Zfs6NL_PGei7xP885CU8tdtL8zaGsQPIZuDxU=s64","userId":"03125645667532537279"}},"outputId":"bcc79ae4-98fd-401a-8303-e6516228c6e6"},"source":["# Let's stick to one target intent.\n","target_intent = \"AddToPlaylist\"\n","\n","# This intent has the following slots\n","target_intent_slot_names = set()\n","for sample in train_data:\n","    if sample['intent'] == target_intent:\n","        for slot_name in sample['slots']:\n","            target_intent_slot_names.add(slot_name)\n","print(target_intent_slot_names)\n","\n","\n","# Extract all the relevant questions of this target intent from the test examples.\n","target_intent_questions = [] \n","for i, question in enumerate(test_questions):\n","    if test_answers[i]['intent'] == target_intent:\n","        target_intent_questions.append(question)\n","print(len(target_intent_questions))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'playlist_owner', 'music_item', 'artist', 'playlist', 'entity_name'}\n","100\n"]}]},{"cell_type":"code","metadata":{"id":"A7_ldSKob6X8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639716900142,"user_tz":300,"elapsed":6,"user":{"displayName":"Parsa Yadollahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnW4Zfs6NL_PGei7xP885CU8tdtL8zaGsQPIZuDxU=s64","userId":"03125645667532537279"}},"outputId":"7463c8a1-8fb7-4c72-bbd7-05d9ec9a50dd"},"source":["def initialize_slots():\n","    slots = {}\n","    for slot_name in target_intent_slot_names:\n","        slots[slot_name] = None\n","    return slots\n","\n","def predict_slot_values(question):\n","    slots = initialize_slots()    \n","    for slot_name in target_intent_slot_names:\n","        # Fill in your code to idenfity the slot value. By default, they are initialized to None.\n","        pass\n","    return slots\n","\n","def evaluate_slot_prediction_recall(slot_prediction_function):\n","    correct = Counter()\n","    total = Counter()\n","    # predict slots for each question\n","    for i, question in enumerate(target_intent_questions):\n","        i = test_questions.index(question) # This line is added after the assignment release\n","        gold_slots = test_answers[i]['slots']\n","        predicted_slots = slot_prediction_function(question)\n","        for name in target_intent_slot_names:\n","            if name in gold_slots:\n","                total[name] += 1.0\n","                if predicted_slots.get(name, None) != None and predicted_slots.get(name).lower() == gold_slots.get(name).lower(): # This line is updated after the assignment release\n","                    correct[name] += 1.0\n","    for name in target_intent_slot_names:\n","        print(f\"{name}: {correct[name] / total[name]}\")\n","\n","\n","# Our reference implementation got these numbers. You can ask others on Slack what they got.\n","# music_item 1.0\n","# playlist 0.67\n","# artist  0.021739130434782608\n","# playlist_owner 0.9444444444444444\n","# entity_name 0.05555555555555555\n","print(\"Slot accuracy for your slot prediction model\")\n","evaluate_slot_prediction_recall(predict_slot_values)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Slot accuracy for your slot prediction model\n","playlist_owner: 0.0\n","music_item: 0.0\n","artist: 0.0\n","playlist: 0.0\n","entity_name: 0.0\n"]}]},{"cell_type":"code","metadata":{"id":"Lm4reFpdb6X8"},"source":["# Find a true positive prediction for each slot\n","# Fill in your code below along with printing your prediction and gold answer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TUo4NblMb6X8"},"source":["# Find a false positive prediction for each slot\n","# Fill in your code below along with print statement\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQjb1-TCb6X8"},"source":["# Find a true negative prediction for each slot\n","# Fill in your code below along with a print statement\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJHTfEMqb6X8"},"source":["# Find a false negative prediction for each slot\n","# Fill in your code below along with a print statement\n"],"execution_count":null,"outputs":[]}]}