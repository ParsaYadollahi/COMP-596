{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 596 - A3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JVkL6n6jU1LU",
        "fk-pkKhvWTMA",
        "gilMYo2eWTWD",
        "HtEdR0wjZ-JN",
        "8hE7VtK5a0m5",
        "aWIwJM95IsLZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "# 1. Vector Space Model of Word Meaning\n",
        "\n",
        "The goal of this problem set is to make you familiar with vector space model of word meaning. You may reuse some of functions you coded in Assignment 1. \n",
        "\n",
        "### Warning: This assignment may take substantial time to run if you are not optimizing your code. Make sure you have plenty of time to run if you are new to programming.\n",
        "\n",
        "Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n",
        "\n",
        "Caution: Since this is real language on Twitter and deals with current events, some of it could be disturbing. In the later section of the course, we will deal with ethics and what is appropriate and what is not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "*Copy function may not work. If so, manually copy the authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0912d452-fd86-4be5-a4e2-4f96a2558a62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see the files in the tweets folder.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99d3441-ad44-4a2d-88db-20612c91bc5f"
      },
      "source": [
        "!ls \"/content/drive/My Drive/tweets\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000_tweets.jsonl\n",
            "20000_tweets.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.tokenized.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.trigrams.txt\n",
            "GoogleNews-vectors-negative300.bin.gz\n",
            "stop_words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0254cdb-bf40-46db-e001-5fc340400a52"
      },
      "source": [
        "# let's read tweets. These tweets are already tokenized and cleaned (Assignment 1)\n",
        "tweets = open(\"/content/drive/My Drive/tweets/covid-tweets-2020-08-10-2020-08-21.tokenized.txt\", \"r\").read().split(\"\\n\")\n",
        "tweets = [tweet.split() for tweet in tweets]\n",
        "\n",
        "print(len(tweets))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkL6n6jU1LU"
      },
      "source": [
        "## Problem 1.1: Word space model\n",
        "\n",
        "Compute the most important context words of `ventilator`. Use Pointwise Mutual Information (PMI) to rank the context words (Refer to Lecture 4).\n",
        "\n",
        "We define context as up to 3 words to the left and 3 words to the right. Ignore stop words and words that do not start with [a-z#]. Also ignore words that are not in the top 1000 frequent words.\n",
        "\n",
        "These context words define the dimensions of the vector space model. Represent each word as a vector (dictionary/counter) of context words with PMI as the importance of the context word. Print the top 20 context words for each.\n",
        "\n",
        "This is the sample output I got for `ventilator`. Your numbers need not match mine but the ranked list should look close to what I have.\n",
        "\n",
        "```\n",
        "[('put', 18.280538283196606), ('wearing', 17.587373569812726), ('even', 17.58651933524197), ('like', 17.402738298715878), ('covid', 17.172590097063086), ('patients', 16.894419647496004), ('use', 16.894298589380956), ('die', 16.89426559608771), ('days', 16.89415252713107), ('needed', 16.489137134110106), ('month', 16.48907033839664), ('weeks', 16.488913820220848), ('away', 16.48879303327717), ('week', 16.488739054051933), ('person', 16.488678720881293), ('good', 16.488160838026904), ('deaths', 16.487822204799755), ('go', 16.487564042558112), ('would', 16.48707075078768), ('one', 16.48706217686235), ('get', 16.486565870239033)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0SbP-yfiaL3"
      },
      "source": [
        "Let's first load stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFoeDtZPTy-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584f5f91-2800-48f3-b0db-c57cd084e51a"
      },
      "source": [
        "stop_words = set()\n",
        "def load_stop_words():\n",
        "  words = open(\"/content/drive/My Drive/tweets/stop_words.txt\", \"r\").read().split(\"\\n\")\n",
        "  for word in words:\n",
        "    stop_words.add(word.strip())\n",
        "\n",
        "load_stop_words()\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "{'doing', 'here', 'such', 'their', 'down', 'only', 'above', 'me', 'yourselves', 'then', 'you', 'with', 'by', 'my', 'and', 'than', 'whom', 'to', 't', 'i', 'during', 'again', 'once', 'each', '@USER', 'our', 'myself', 'these', \"'ld\", 'why', 'more', 'who', \"'ve\", 'other', 'are', 'up', 'is', 'which', 'yourself', 'will', 'hers', 'on', 'because', 'or', 'below', 'until', 'further', 'being', 'through', 'while', 'had', 'under', 'don', 'were', 'your', 'what', 'same', 'having', 'they', 'the', 'should', 'any', 'its', 'ours', 'an', 'just', 'over', 'she', 'how', 'himself', 'own', 'URL', 'from', 'be', 'where', \"n't\", 'into', 'he', 'of', 'so', 'as', 'ourselves', 'it', 'very', 'theirs', 'out', 'both', 'few', 'now', 'herself', 'too', 's', 'for', 'against', 'some', \"'ll\", 'itself', 'but', 'those', 'have', 'a', 'has', 'when', 'all', 'amp', 'that', 'do', 'yours', 'there', 'we', 'off', 'nor', 'not', 'after', 'her', 'most', 'am', 'this', 'did', 'before', 'between', 'his', 'at', 'them', 'him', 'about', 'does', 'was', 'been', 'can', 'if', 'themselves', 'in', 'no'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJF6PXSHQBiR"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Let's build the word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYKk9MzY2PMs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I_T8bcDUqFD",
        "outputId": "79b431ea-03f6-437d-fa3f-fb4cd3e0285f"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pformat\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds, norm\n",
        "from nltk.util import skipgrams\n",
        "from string import punctuation\n",
        "from scipy import sparse\n",
        "from scipy.sparse import linalg\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import sparse\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "stemmer= PorterStemmer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao34mmYmUtNL"
      },
      "source": [
        "unigram_counts = Counter()\n",
        "skipgram_counts = Counter()\n",
        "x2i, i2x = {}, {}\n",
        "\n",
        "tweets_trimmed = [[token for token in tweet if token not in stop_words and (re.match(r\"[a-z#]+\", token[0]))] for tweet in tweets]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kd9h8A0Uywb",
        "outputId": "d9ea1067-0fce-42ad-c001-70eba75dddc5"
      },
      "source": [
        "def compute_unigrams():\n",
        "  global unigram_counts\n",
        "  unigram_counts = Counter()\n",
        "  for idx, tweet in enumerate(tweets_trimmed):\n",
        "    for token in tweet:\n",
        "      unigram_counts[token] += 1\n",
        "\n",
        "  # Remove all unigrams in in top 1000\n",
        "  most_common_unigram = unigram_counts.most_common(1000)\n",
        "  for k,v in list(unigram_counts.copy().items()):\n",
        "    if (k,v) not in most_common_unigram:\n",
        "      del unigram_counts[k]\n",
        "  print('Most common unigram: {}'.format(unigram_counts.most_common(10)))\n",
        "  print('Trimmed vocab size should be 1000: {}'.format(len(unigram_counts)))\n",
        "\n",
        "compute_unigrams()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common unigram: [('covid', 71281), ('pandemic', 50353), ('covid-19', 33591), ('people', 31850), ('n’t', 31053), ('like', 20837), ('mask', 20107), ('get', 19982), ('coronavirus', 19949), ('trump', 19223)]\n",
            "Trimmed vocab size should be 1000: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rShrYCWbU17M",
        "outputId": "c9a519a2-6ea3-41df-f49d-9eb47a2050eb"
      },
      "source": [
        "def compute_skip_grams():\n",
        "  global skipgram_counts\n",
        "  for idx, tweet in enumerate(tweets_trimmed):\n",
        "      # 3 words left and right\n",
        "      skip_gram = list(skipgrams(tweet, 2, 3))\n",
        "      for context in skip_gram:\n",
        "        skipgram_counts[context] += 1\n",
        "\n",
        "  print('Skipgram size: {}'.format(len(skipgram_counts)))\n",
        "  print('Most common skipgrams: {}'.format(skipgram_counts.most_common(10)))\n",
        "\n",
        "compute_skip_grams()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipgram size: 6209898\n",
            "Most common skipgrams: [(('wear', 'mask'), 11991), (('social', 'distancing'), 6776), (('stay', 'home'), 6009), (('ca', 'n’t'), 4990), (('new', 'cases'), 4106), (('gon', 'na'), 2887), (('cases', 'deaths'), 2547), (('people', 'covid'), 2362), (('get', 'covid'), 2358), (('covid', 'n’t'), 2278)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL4FouYIUUi6"
      },
      "source": [
        "def matrix_word(words):\n",
        "  f = np.hstack(words)\n",
        "  freq = Counter(f)\n",
        "  top_freq = sorted(list(freq.items()), key=lambda x: x[1], reverse=True)[:1000]\n",
        "  index_word = {top_freq[i][0]: i for i in range(len(top_freq))}\n",
        "\n",
        "  return freq, index_word\n",
        "freq, index_word = matrix_word(tweets_trimmed)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG3Fa4ecVvrf"
      },
      "source": [
        "tot_words = 0\n",
        "for tweet in tweets_trimmed:\n",
        "  tot_words += len(tweet)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfCEPjaGUEjH",
        "outputId": "7e31d280-350e-4319-8943-517a7f70bbc5"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def PMI(word1, word2, join_freq):\n",
        "  # Write your code\n",
        "  cx = freq[word1]\n",
        "  cy = freq[word2]\n",
        "  cxy = join_freq[index_word[word2]] * tot_words\n",
        "\n",
        "  if cxy:\n",
        "    return log(cxy / cx / cy)\n",
        "  return 0\n",
        " \n",
        "def build_word_vector(word):\n",
        "  # Write your code \n",
        "  join_freq = [0] * 1000 \n",
        "  for tweet in tweets_trimmed:\n",
        "    if word not in tweet:\n",
        "      continue\n",
        "    for ind in range(len(tweet)):\n",
        "      if tweet[ind] != word:\n",
        "        continue\n",
        "      for dx in range(ind -3, ind+4):\n",
        "        if (dx >= 0 and dx < len(tweet) and dx != ind) and (tweet[dx] in index_word.keys()):\n",
        "          join_freq[index_word[tweet[dx]]] += 1\n",
        "  for k in index_word.keys():\n",
        "    join_freq[index_word[k]] = PMI(word, k, join_freq)\n",
        "  \n",
        "  return join_freq\n",
        "\n",
        "def print_top_dimensions(word_vector, n):\n",
        "  # print top n dimensions sorted in the order of importance.\n",
        "  rev_idx = {v: k for k, v in index_word.items()}\n",
        "  word_vector_pro = {rev_idx[i]: word_vector[i] for i in range(len(word_vector))}\n",
        "  sorted_word_vec = sorted(list(word_vector_pro.items()), key=lambda x: x[1], reverse=True)\n",
        "  print(sorted_word_vec[:n])\n",
        "\n",
        "\n",
        "v1 = build_word_vector('ventilator')\n",
        "print_top_dimensions(v1, 20) # print top 20 dimensions along with their weights"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('patient', 5.411850239784782), ('hospital', 4.743324143615185), ('patients', 4.667741462449178), ('put', 4.591270340211511), ('spent', 4.318976847785691), ('enjoy', 4.282249556065533), ('tried', 4.268475233600704), ('loved', 4.217797723844838), ('end', 4.202171285881991), ('brain', 4.162157583802672), ('critical', 4.121187998698428), ('increase', 4.100737388227137), ('needed', 4.042872328676352), ('days', 4.031582586531751), ('month', 4.028779954536644), ('experience', 4.00853419109546), ('weeks', 3.9578036429242243), ('failure', 3.928491483421924), ('k', 3.8113217123362664), ('wearing', 3.643490704471721)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLZlAXuUVQXj"
      },
      "source": [
        "## Problem 1.2: Compute the word similarity between words using Cosine Similarity.\n",
        "\n",
        "Compute cosine similarity between the following pair of words: \n",
        "```\n",
        "('ventilator', 'covid-19')\n",
        "('ventilator', 'lockdown')\n",
        "('ventilator', 'mask')\n",
        "('ventilator', 'ppe')\n",
        "```\n",
        "\n",
        "Outputs of my code are:\n",
        "```\n",
        "('ventilator', 'covid-19') 0.17076006036635358\n",
        "('ventilator', 'lockdown') ...\n",
        "('ventilator', 'mask') 0.19229601085517933\n",
        "('ventilator', 'ppe') ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4x3CVEnsctn"
      },
      "source": [
        "ventilator = build_word_vector('ventilator')\n",
        "covid19 = build_word_vector('covid-19')\n",
        "lockdown = build_word_vector('lockdown')\n",
        "mask = build_word_vector('mask')\n",
        "ppe = build_word_vector('ppe')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A3OJtOuXH5t",
        "outputId": "4297b303-6d9c-4b2c-aa97-a84f99c8e0ec"
      },
      "source": [
        "from numpy import linalg as LNG \n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "  cos, cxy, cx, cy = 0, 0, 0, 0\n",
        "\n",
        "  for i in range(len(vector1)):\n",
        "    cx += vector1[i] ** 2\n",
        "    cy += vector2[i] ** 2\n",
        "    cxy += vector1[i] * vector2[i]\n",
        "  cos = cxy/(cx + cy)\n",
        "  return cos\n",
        "\n",
        "print(('ventilator', 'covid-19'), cosine_similarity(ventilator, covid19))\n",
        "print(('ventilator', 'lockdown'), cosine_similarity(ventilator, lockdown))\n",
        "print(('ventilator', 'mask'), cosine_similarity(ventilator, mask))\n",
        "print(('ventilator', 'ppe'), cosine_similarity(ventilator, ppe))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ventilator', 'covid-19') 0.21577910124582148\n",
            "('ventilator', 'lockdown') 0.21327248533218487\n",
            "('ventilator', 'mask') 0.1822954530789848\n",
            "('ventilator', 'ppe') 0.1884722830013833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJHApLrmLnw"
      },
      "source": [
        "## Problem 1.3: What can you tell about these words from the similarities?\n",
        "\n",
        "1. `ventilator` when compared with `covid-19, lockdown, mask, ppe`\n",
        "2. `pandemic` when compared with `covid-19, lockdown, mask, ppe`\n",
        "3. `president` compared with `trump, biden`\n",
        "4. `trudeau` compared with `trump, biden`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olK_XHXLGwKT",
        "outputId": "1d569721-98b4-4fe0-d4b2-7e4ab2fe895b"
      },
      "source": [
        "ventilator = build_word_vector('ventilator')\n",
        "pandemic = build_word_vector('pandemic')\n",
        "president = build_word_vector('president')\n",
        "trudeau = build_word_vector('trudeau')\n",
        "\n",
        "covid19 = build_word_vector('covid-19')\n",
        "lockdown = build_word_vector('lockdown')\n",
        "mask = build_word_vector('mask')\n",
        "ppe = build_word_vector('ppe')\n",
        "trump = build_word_vector('trump')\n",
        "biden = build_word_vector('biden')\n",
        "\n",
        "\n",
        "print(('ventilator', 'covid-19'), cosine_similarity(ventilator, covid19))\n",
        "print(('ventilator', 'lockdown'), cosine_similarity(ventilator, lockdown))\n",
        "print(('ventilator', 'mask'), cosine_similarity(ventilator, mask))\n",
        "print(('ventilator', 'ppe'), cosine_similarity(ventilator, ppe))\n",
        "print()\n",
        "print(('pandemic', 'covid-19'), cosine_similarity(pandemic, covid19))\n",
        "print(('pandemic', 'lockdown'), cosine_similarity(pandemic, lockdown))\n",
        "print(('pandemic', 'mask'), cosine_similarity(pandemic, mask))\n",
        "print(('pandemic', 'ppe'), cosine_similarity(pandemic, ppe))\n",
        "print()\n",
        "print(('president', 'trump'), cosine_similarity(president, trump))\n",
        "print(('president', 'biden'), cosine_similarity(president, biden))\n",
        "print()\n",
        "print(('trudeau', 'trump'), cosine_similarity(trudeau, trump))\n",
        "print(('trudeau', 'biden'), cosine_similarity(trudeau, biden))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ventilator', 'covid-19') 0.21577910124582148\n",
            "('ventilator', 'lockdown') 0.21327248533218487\n",
            "('ventilator', 'mask') 0.1822954530789848\n",
            "('ventilator', 'ppe') 0.1884722830013833\n",
            "\n",
            "('pandemic', 'covid-19') 0.41007794642634\n",
            "('pandemic', 'lockdown') 0.4251049786200557\n",
            "('pandemic', 'mask') 0.3526730125403971\n",
            "('pandemic', 'ppe') 0.3180970573197574\n",
            "\n",
            "('president', 'trump') 0.4537076610848569\n",
            "('president', 'biden') 0.41620554983315994\n",
            "\n",
            "('trudeau', 'trump') 0.2673053910979366\n",
            "('trudeau', 'biden') 0.2533337977801984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWNpJewZrMJb"
      },
      "source": [
        "1. 'covid-19', 'lockdown', 'mask' and 'ppe' are equivalently close to the context word 'ventilator' but 'covid-19' and 'lockdown' are slighlty closer\n",
        "2. Here, we notice that 'ppe' is the least relevant to 'pandemic' since it has the smallest cosine similarity whereas 'covid-19', 'lockdown' and 'mask' are all relatively close to each other.\n",
        "3. 'trump' and 'biden' are equivalently close to the context word 'president'\n",
        "4. 'trump' and 'biden' are equivelently close to the context word' trudeau' and more than 'president'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-FpH2k7akG"
      },
      "source": [
        "# Let's play with word2vec\n",
        "\n",
        "First let's load word2vec. I am using [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) but feel free to use any libraries or tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnAsRw6CiIY6"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Jcaf1T9mHU"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "EMBEDDING_FILE = '/content/drive/My Drive/tweets/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXHjFq6upAUA"
      },
      "source": [
        "## Problem 1.4: Compute the top 5 similar words of `ventilator` using word2vec?\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvff7yDBFUMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816347c9-f7aa-455b-d04e-a81ea3ead72c"
      },
      "source": [
        "# Write your code here\n",
        "word2vec.most_similar(['ventilator'] , topn = 5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('respirator', 0.7864563465118408),\n",
              " ('mechanical_ventilator', 0.7063840627670288),\n",
              " ('intensive_care', 0.6809945702552795),\n",
              " ('ventilators', 0.6683582067489624),\n",
              " ('breathing_tube', 0.6665509343147278)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufUX14RV2bCY"
      },
      "source": [
        "# Problem 1.5: Word analogy\n",
        "\n",
        "If I told you the plural of `car` is `cars`, can you automatically find the plural of `hypothesis` and `man` using word2vec.\n",
        "\n",
        "Similarly, if I told you a newborn `dog` is called `puppy`, can you automatically find what are the newborn words of `cat` and `sheep` using word2vec?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWJjb0Vh61JN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cba67d5-c96d-466f-85bf-077ca46238ef"
      },
      "source": [
        "print(word2vec.most_similar(positive=['hypothesis','cars'], negative=['car'] )[0][0])\n",
        "print(word2vec.most_similar(positive=['man','cars'], negative=['car'] )[0][0])\n",
        "\n",
        "print(word2vec.most_similar(positive=['cat','puppy'], negative=['dog'] )[0][0])\n",
        "print(word2vec.most_similar(positive=['sheep','puppy'], negative=['dog'] )[0][0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hypotheses\n",
            "men\n",
            "kitten\n",
            "lambs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXX-DQe2oTcH"
      },
      "source": [
        "# 2. Topic Models \n",
        "\n",
        "The goal of this part is to make you familiar with topic models. You may reuse some of functions you coded for the previous assignments.\n",
        "\n",
        "## Data Download and Setup\n",
        "\n",
        "Let us start by downloading the news section of the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrStv7GPOixj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb736f8-ce05-4004-9cfc-fab160ea1bf5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "documents = [brown.words(fileid) for fileid in brown.fileids()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m5pYgzSqJIq"
      },
      "source": [
        "Let us inspect some of the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvgANNlUOjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43674777-c20c-410a-85e8-a1f61d7f688e"
      },
      "source": [
        "print(\"The news section of the Brown corpus contains {} documents.\".format(len(documents)))\n",
        "for i in range(3):\n",
        "  document = documents[i]\n",
        "  print(\"Document {} has {} words: {}\".format(i, len(document), document))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The news section of the Brown corpus contains 500 documents.\n",
            "Document 0 has 2242 words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "Document 1 has 2277 words: ['Austin', ',', 'Texas', '--', 'Committee', 'approval', ...]\n",
            "Document 2 has 2275 words: ['Several', 'defendants', 'in', 'the', 'Summerdale', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfx34Lg4Vi9l"
      },
      "source": [
        "Finally, let us download a list of stopwords for later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlizEa3mVnWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00c35e0-3b60-4cdc-fe9d-cbbf7884bb59"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "print(stopwords_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk-pkKhvWTMA"
      },
      "source": [
        "## Problem 2.1: Document-Term Matrix\n",
        "\n",
        "Create a document-term matrix with tf-idf. You should preprocess documents by: 1) lowercasing words, 2) excluding stopwords, and 3) including alphanumeric strings only.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "def create_tfidf_matrix(documents: List[List[str]]) -> (np.array, List[str]):\n",
        "  # Args:\n",
        "  #   documents: list of documents, each document being a list of words.\n",
        "  # Outputs:\n",
        "  #   tfidf_matrix: np.array of shape (num_documents, vocabulary_size)\n",
        "  #   vocabulary: a list of terms corresponding to the columns of the matrix.\n",
        "\n",
        "tfidf_matrix, vocabulary = create_tfidf_matrix(documents)\n",
        "```\n",
        "\n",
        "How sparse is this matrix? Calculate the ratio between cells with value 0 and the total number of cells. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmQcVe96vzQx"
      },
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "def preprocess(documents: List[List[str]]):\n",
        "  return_docs = []\n",
        "  documents_processed = [[token.lower() for token in document if token not in stopwords_list and token.isalnum()] for document in documents]\n",
        "  return documents_processed\n",
        "  \n",
        "documents_processed = preprocess(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhYfubRLFOBZ"
      },
      "source": [
        "def compute_word_set(documents):\n",
        "  wordset = set()\n",
        "  for document in documents:\n",
        "    wordset = wordset.union(set(document))\n",
        "  return wordset\n",
        "\n",
        "wordset = compute_word_set(documents_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3jkxqVhGC6z"
      },
      "source": [
        "def compute_word_dict(documents):\n",
        "  global wordset\n",
        "  word_dict_list = []\n",
        "  for document in documents:\n",
        "    word_dict = dict.fromkeys(wordset, 0)\n",
        "    for word in document:\n",
        "      word_dict[word] += 1\n",
        "    word_dict_list.append(word_dict)\n",
        "  return word_dict_list\n",
        "\n",
        "word_dict_list = compute_word_dict(documents_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0kDLNPEHo9p"
      },
      "source": [
        "def computeTF(wordDict, bow):\n",
        "    tfDict = {}\n",
        "    bowCount = len(bow)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count/float(bowCount)\n",
        "    return tfDict\n",
        "\n",
        "tf = []\n",
        "for idx, doc in enumerate(documents_processed):\n",
        "  tf.append(computeTF(word_dict_list[idx], doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5M5TVUcKI-T"
      },
      "source": [
        "def computeIDF(docList):\n",
        "    import math\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "    \n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "        for word, val in doc.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log10(N / float(val))\n",
        "        \n",
        "    return idfDict\n",
        "\n",
        "idfs = computeIDF(tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkuo2iWHKe9G"
      },
      "source": [
        "def computeTFIDF(tfBow, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBow.items():\n",
        "        tfidf[word] = val*idfs[word]\n",
        "    return tfidf\n",
        "\n",
        "tfidf = []\n",
        "for i in tf:\n",
        "  tfidf.append(computeTFIDF(i, idfs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5coT5h55K6cu",
        "outputId": "de242698-669a-4855-94dc-28b9a2b44fea"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_tfidf_matrix(documents: List[List[str]]) -> (np.array, List[str]):\n",
        "  # Args:\n",
        "  #   documents: list of documents, each document being a list of words.\n",
        "  # Outputs:\n",
        "  #   tfidf_matrix: np.array of shape (num_documents, vocabulary_size)\n",
        "  #   vocabulary: a list of terms corresponding to the columns of the matrix.\n",
        "\n",
        "  global tfidf\n",
        "  tfidfDF = pd.DataFrame(tfidf)\n",
        "  tfidf_matrix = pd.DataFrame(tfidfDF).to_numpy()\n",
        "  voc = list(tfidfDF.columns.values)\n",
        "  return (tfidf_matrix, voc)\n",
        "\n",
        "tfidf_matrix, vocabulary =  create_tfidf_matrix(documents_processed)\n",
        "sparsity = 1.0 - ( count_nonzero(tfidf_matrix) / float(tfidf_matrix.size) )\n",
        "print(\"Sparcity of a matrix is {}\".format(sparsity))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparcity of a matrix is 0.983964396322579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gilMYo2eWTWD"
      },
      "source": [
        "## Problem 2.2: Latent Semantic Analysis\n",
        "\n",
        "We perform LSA to obtain document embeddings `U` and term embeddings `VT`.\n",
        "\n",
        "```python\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(tfidf_matrix, \n",
        "                              n_components=10,\n",
        "                              n_iter=100,\n",
        "                              random_state=42)\n",
        "```\n",
        "\n",
        "Define a function to find the 5 most relevant terms for each of the 10 latent dimensions (tip: you should make use of VT and the vocabulary).\n",
        "\n",
        "```python\n",
        "def extract_salient_words(VT: np.array, \n",
        "                  vocabulary: List[str]\n",
        "                  ) -> salient_words: dict[int, List[str]]:\n",
        "  # Args:\n",
        "  #  VT: a numpy array of size (n_components, vocabulary_size)\n",
        "  #  vocabulary: a list of words of size vocabulary_size\n",
        "  # Outputs:\n",
        "  #  salient_words: a dictionary with the latent dimension indices as keys and a list of its 5 most salient words as values\n",
        "\n",
        "salient_words = extract_salient_words(VT, vocabulary)\n",
        "\n",
        "for key, value in salient_words:\n",
        "  print(\"Concept {}: {}\".format(str(key), \" \".join(value)))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMe5SFAtkGRn"
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(tfidf_matrix, \n",
        "                              n_components=10,\n",
        "                              n_iter=100,\n",
        "                              random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Amv_EdXkH5y",
        "outputId": "8147efa4-f767-4287-df00-88ae97bac535"
      },
      "source": [
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "\n",
        "def extract_salient_words(VT: np.array, vocabulary: List[str]) -> Dict[int, List[str]]:\n",
        "  # Args:\n",
        "  #  VT: a numpy array of size (n_components, vocabulary_size)\n",
        "  #  vocabulary: a list of words of size vocabulary_size\n",
        "  # Outputs:\n",
        "  #  salient_words: a dictionary with the latent dimension indices as keys and a list of its 5 most salient words as values\n",
        "  salient_words: Dict[int, List[str]] = {}\n",
        "  for i in range(len(VT)):\n",
        "    tfidf_sorting = np.argsort(VT[i]).flatten()[::-1]\n",
        "    n = 5\n",
        "    voc = np.array(vocabulary)\n",
        "    top_n = voc[tfidf_sorting]\n",
        "    salient_words[i] = top_n[:n].tolist()\n",
        "  return salient_words\n",
        "\n",
        "salient_words = extract_salient_words(VT, vocabulary)\n",
        "for key, value in salient_words.items():\n",
        "  print(\"Concept {}: {}\".format(str(key), \" \".join(value)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concept 0: af t polynomial operator q\n",
            "Concept 1: af t polynomial operator theorem\n",
            "Concept 2: anode holder arc temperature plug\n",
            "Concept 3: anode i she phil he\n",
            "Concept 4: staining nonspecific clover sections wtv\n",
            "Concept 5: dictionary text cell information forms\n",
            "Concept 6: platform gyro accelerometer leveling axis\n",
            "Concept 7: q secants tangent curve secant\n",
            "Concept 8: feed bronchial artery milligrams pulmonary\n",
            "Concept 9: bronchial artery pulmonary skywave bronchioles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEdR0wjZ-JN"
      },
      "source": [
        "## Problem 2.3: Document Retrieval\n",
        "\n",
        "Given a text query, view this as a mini document, and compare it to your documents in the low-dimensional space.\n",
        "\n",
        "First, we need to map a query into a pseudo-document embedding.\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ed5d0397ee6b44f72f77743029d3943932118fa2\" alt=\"Query\" height=\"35\"/>\n",
        "\n",
        "Then, you will need to implement a function to calculate the cosine similarity between this embedded query and all the document embeddings.\n",
        "\n",
        "Retrieve the indices of the top-3 documents with the highest cosine similarity with the following queries:\n",
        "\n",
        "\n",
        "```python\n",
        "query1 = ['T.', 'V.', 'Barker', 'developed', 'the', 'classification-angle', 'system']\n",
        "query2 = ['imitation', 'vs.', 'formalism' 'in', 'philosophical', 'debates']\n",
        "query3 = ['Krim', 'attended', 'the', 'University', 'of', 'North', 'Carolina', 'to', 'follow', 'Thomas', 'Wolfe']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hE7VtK5a0m5"
      },
      "source": [
        "## Problem 2.4: Document Clustering\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "num_clusters = 10\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "document_embeddings = U * Sigma\n",
        "km.fit(document_embeddings)\n",
        "clusters = km.labels_.tolist()\n",
        "print(clusters)\n",
        "```\n",
        "\n",
        "Let us now plot the document embeddings and their clusters:\n",
        "\n",
        "```python\n",
        "import umap\n",
        "embedding = umap.UMAP(n_neighbors=100, min_dist=0.5, random_state=42).fit_transform(document_embeddings)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=20, edgecolor='none')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "What are the differences you observe by using a different number of `n_components` in LSA or `n_clusters` in K-Means?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iok1VFGS8Q2V"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN2a4xSJAP8_"
      },
      "source": [
        "num_clusters = 20\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "document_embeddings = U * Sigma\n",
        "km.fit(document_embeddings)\n",
        "clusters = km.labels_.tolist()\n",
        "print(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4NA6pa38SQB"
      },
      "source": [
        "embedding = umap.UMAP(n_neighbors=100, min_dist=0.5, random_state=42).fit_transform(document_embeddings)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=20, edgecolor='none')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIwJM95IsLZ"
      },
      "source": [
        "## Problem 2.5 Latent Dirichlet Allocation\n",
        "\n",
        "Run LDA on `documents` using `sklearn` (find the documentation at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation))\n",
        "\n",
        "Make sure to specify `random_state=42` for replicability. \n",
        "\n",
        "What are the topics allocated to each word of document number 13? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc-j5rsLJaeX"
      },
      "source": [
        "print(documents[13])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0KIkqOqD8H"
      },
      "source": [
        "# Write your code here.\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "\n",
        "\n",
        "X, _ = make_multilabel_classification(random_state=0)\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "\n",
        "lda.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}