{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 596 - A3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "# 1. Vector Space Model of Word Meaning\n",
        "\n",
        "The goal of this problem set is to make you familiar with vector space model of word meaning. You may reuse some of functions you coded in Assignment 1. \n",
        "\n",
        "### Warning: This assignment may take substantial time to run if you are not optimizing your code. Make sure you have plenty of time to run if you are new to programming.\n",
        "\n",
        "Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n",
        "\n",
        "Caution: Since this is real language on Twitter and deals with current events, some of it could be disturbing. In the later section of the course, we will deal with ethics and what is appropriate and what is not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "*Copy function may not work. If so, manually copy the authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8227c286-a815-4eb6-b5cf-330c5b9532b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see the files in the tweets folder.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9856a43a-1995-47d2-b236-402a99e24e1f"
      },
      "source": [
        "!ls \"/content/drive/My Drive/tweets\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000_tweets.jsonl\n",
            "20000_tweets.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.tokenized.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.trigrams.txt\n",
            "GoogleNews-vectors-negative300.bin.gz\n",
            "stop_words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06513fc5-8630-4ea3-9888-6ce7cc80c448"
      },
      "source": [
        "# let's read tweets. These tweets are already tokenized and cleaned (Assignment 1)\n",
        "tweets = open(\"/content/drive/My Drive/tweets/covid-tweets-2020-08-10-2020-08-21.tokenized.txt\", \"r\").read().split(\"\\n\")\n",
        "tweets = [tweet.split() for tweet in tweets]\n",
        "\n",
        "print(len(tweets))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkL6n6jU1LU"
      },
      "source": [
        "## Problem 1.1: Word space model\n",
        "\n",
        "Compute the most important context words of `ventilator`. Use Pointwise Mutual Information (PMI) to rank the context words (Refer to Lecture 4).\n",
        "\n",
        "We define context as up to 3 words to the left and 3 words to the right. Ignore stop words and words that do not start with [a-z#]. Also ignore words that are not in the top 1000 frequent words.\n",
        "\n",
        "These context words define the dimensions of the vector space model. Represent each word as a vector (dictionary/counter) of context words with PMI as the importance of the context word. Print the top 20 context words for each.\n",
        "\n",
        "This is the sample output I got for `ventilator`. Your numbers need not match mine but the ranked list should look close to what I have.\n",
        "\n",
        "```\n",
        "[('put', 18.280538283196606), ('wearing', 17.587373569812726), ('even', 17.58651933524197), ('like', 17.402738298715878), ('covid', 17.172590097063086), ('patients', 16.894419647496004), ('use', 16.894298589380956), ('die', 16.89426559608771), ('days', 16.89415252713107), ('needed', 16.489137134110106), ('month', 16.48907033839664), ('weeks', 16.488913820220848), ('away', 16.48879303327717), ('week', 16.488739054051933), ('person', 16.488678720881293), ('good', 16.488160838026904), ('deaths', 16.487822204799755), ('go', 16.487564042558112), ('would', 16.48707075078768), ('one', 16.48706217686235), ('get', 16.486565870239033)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0SbP-yfiaL3"
      },
      "source": [
        "Let's first load stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFoeDtZPTy-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67514e5d-9de3-447f-8638-7b0977465002"
      },
      "source": [
        "stop_words = set()\n",
        "def load_stop_words():\n",
        "  words = open(\"/content/drive/My Drive/tweets/stop_words.txt\", \"r\").read().split(\"\\n\")\n",
        "  for word in words:\n",
        "    stop_words.add(word.strip())\n",
        "\n",
        "load_stop_words()\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "{'at', \"'ll\", 'on', 'few', 'to', 'whom', 'how', 'here', 'no', 'them', 'will', 'a', 'themselves', 's', 'such', 'had', 'should', 'most', 'from', 'did', 'has', 'do', 'ours', 'between', 'you', 'before', 'again', 'we', 'myself', 'both', 'himself', 'its', 'him', 'when', 'same', 'doing', 'what', 'so', 'me', 'each', 'she', 'than', 't', 'of', 'further', \"'ld\", 'all', 'theirs', 'hers', 'any', 'through', 'are', 'very', 'after', 'out', 'once', 'is', 'above', 'an', 'your', '@USER', 'there', 'in', 'URL', 'her', 'just', 'ourselves', 'amp', 'which', 'am', 'down', 'having', 'off', 'yourselves', 'can', 'too', 'they', 'that', 'being', 'itself', 'i', 'who', 'some', 'own', 'does', 'below', 'been', 'up', 'why', 'against', 'then', 'were', 'be', 'and', 'more', 'about', 'yourself', 'until', 'while', 'with', \"n't\", 'their', 'those', 'or', 'if', 'under', 'the', 'his', 'into', 'over', 'for', 'our', 'not', 'during', 'he', 'yours', 'because', 'but', 'was', 'as', 'where', \"'ve\", 'by', 'have', 'my', 'only', 'herself', 'other', 'don', 'now', 'these', 'it', 'this', 'nor'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJF6PXSHQBiR"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Let's build the word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ896-ClQBKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c878c5-fb85-4676-aad3-3be9d5e41681"
      },
      "source": [
        "# help here: https://www.kaggle.com/gabrielaltay/word-vectors-from-pmi-matrix\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pformat\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds, norm\n",
        "from nltk.util import skipgrams\n",
        "from string import punctuation\n",
        "from scipy import sparse\n",
        "from scipy.sparse import linalg\n",
        "\n",
        "tweets_trimmed = [[token for token in tweet if token not in stop_words] for tweet in tweets]\n",
        "\n",
        "unigram_counts = Counter()\n",
        "skipgram_counts = Counter()\n",
        "x2i, i2x = {}, {}\n",
        "pmi_samples = Counter()\n",
        "\n",
        "\n",
        "\n",
        "def compute_unigrams():\n",
        "  global unigram_counts\n",
        "  for idx, tweet in enumerate(tweets_trimmed):\n",
        "    for token in tweet:\n",
        "      unigram_counts[token] += 1\n",
        "    if idx % 50000 == 0:\n",
        "        print(f'finished {idx/len(tweets):.2%} of tweets')\n",
        "\n",
        "  print('Vocab size: {}'.format(len(unigram_counts)))\n",
        "  print('Most common unigrams: {}'.format(unigram_counts.most_common(10)))\n",
        "  most_common_unigram = unigram_counts.most_common(10000)\n",
        "  # print(unigram_counts['ventilator'])\n",
        "  # print(unigram_counts.most_common(10000)[9999])\n",
        "  for k,v in list(unigram_counts.items()):\n",
        "    if (k,v) not in most_common_unigram:\n",
        "      del unigram_counts[k]\n",
        "\n",
        "  print('NEW vocab size: {}'.format(len(unigram_counts)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_skip_grams():\n",
        "  global skipgram_counts\n",
        "  for idx, tweet in enumerate(tweets_trimmed):\n",
        "      # 3 words left and right\n",
        "      skip_gram = list(skipgrams(tweet, 2, 2))\n",
        "      for context in skip_gram:\n",
        "        skipgram_counts[context] += 1\n",
        "      if idx % 50000 == 0:\n",
        "        print(f'finished {idx/len(tweets_trimmed):.2%} of tweets')\n",
        "\n",
        "\n",
        "  print('Skipgram size: {}'.format(len(skipgram_counts)))\n",
        "  print('Most common skipgrams: {}'.format(skipgram_counts.most_common(10)))\n",
        "\n",
        "\n",
        "# def remove_freq_infreq_unigrams():\n",
        "#   print('unigrms before: {}'.format(len(unigram_counts)))\n",
        "#   min_count = (1 / 1000) * len(tweets)\n",
        "#   max_count = (1 / 50) * len(tweets)\n",
        "#   for unigram in list(unigram_counts.keys()):\n",
        "#     if unigram_counts[unigram] < 1000 or unigram_counts[unigram] > max_count:\n",
        "#       del unigram_counts[unigram]\n",
        "\n",
        "\n",
        "#   print('unigrams after: {}'.format(len(unigram_counts)))\n",
        "#   print('NEW Most common: {}'.format(unigram_counts.most_common(10)))\n",
        "\n",
        "def remove_infreq_bigrams():\n",
        "  print('Skipgram before: {}'.format(len(skipgram_counts)))\n",
        "  for x, y in list(skipgram_counts.keys()):\n",
        "    if x not in unigram_counts or y not in unigram_counts:\n",
        "      del skipgram_counts[(x,y)]\n",
        "\n",
        "  print('Skipgram after: {}'.format(len(skipgram_counts)))\n",
        "  print('NEW Most common skipgram: {}'.format(skipgram_counts.most_common(10)))\n",
        "\n",
        "\n",
        "def build_unigram_index_lookup():\n",
        "  global x2i\n",
        "  global i2x\n",
        "  for i, x in enumerate(unigram_counts.keys()):\n",
        "    x2i[x] = i\n",
        "    i2x[i] = x\n",
        "    \n",
        "\n",
        "    \n",
        "def PMI(word1, word2, sx, sxy):\n",
        "  # You have to store frequencies of individual words and (word, context word) \n",
        "  # pairs to compute this. You can compute them beforehand in order to avoid \n",
        "  # counting every time when this function is called.\n",
        "  # Write your code\n",
        "  global skipgram_counts\n",
        "  global unigram_counts\n",
        "  a = (skipgram_counts[(word1,word2)] / sxy)\n",
        "  b = (unigram_counts[word1] / sx)\n",
        "  c = (unigram_counts[word2] / sx)\n",
        "  return log(a / b / c)\n",
        "\n",
        "def build_word_vector(word):\n",
        "  # Write your code\n",
        "  global skipgram_counts\n",
        "  global unigram_counts\n",
        "  global x2i\n",
        "  global i2x\n",
        "  sx = sum(unigram_counts.values())\n",
        "  sxy = sum(skipgram_counts.values())\n",
        "  data, rows, cols = [], [], []\n",
        "  pmi_sample = Counter()\n",
        "\n",
        "  for (x, y), n in skipgram_counts.items():\n",
        "    rows.append(x2i[x])\n",
        "    cols.append(x2i[y])\n",
        "    data.append(PMI(x,y, sx, sxy))\n",
        "    pmi_sample[(x,y)] = data[-1]\n",
        "\n",
        "  PMI_matrix = csc_matrix((data, (rows, cols)))\n",
        "  print('%d non-zero elements' % PMI_matrix.count_nonzero())\n",
        "\n",
        "  \n",
        "  for (x,y), n in list(pmi_sample.items()):\n",
        "    if (x != word and y != word):\n",
        "      del pmi_sample[(x,y)]\n",
        "\n",
        "  print(pmi_sample.most_common()[:20])\n",
        "\n",
        "\n",
        "def print_top_dimensions(word_vector, n):\n",
        "  # print top n dimensions sorted in the order of importance.\n",
        "  ...\n",
        "\n",
        "compute_unigrams()\n",
        "print()\n",
        "compute_skip_grams()\n",
        "print()\n",
        "remove_infreq_bigrams()\n",
        "print()\n",
        "build_unigram_index_lookup()\n",
        "# build_sparce_PMI_matrix()\n",
        "v1 = build_word_vector('ventilator')\n",
        "\n",
        "# print_top_dimensions(v1, 20) # print top 20 dimensions along with their weights\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished 0.00% of tweets\n",
            "finished 15.98% of tweets\n",
            "finished 31.96% of tweets\n",
            "finished 47.94% of tweets\n",
            "finished 63.92% of tweets\n",
            "finished 79.90% of tweets\n",
            "finished 95.88% of tweets\n",
            "Vocab size: 159852\n",
            "Most common unigrams: [('covid', 71281), ('pandemic', 50353), ('’s', 34853), (\"'s\", 33911), ('covid-19', 33591), ('people', 31850), ('n’t', 31053), ('like', 20837), ('mask', 20107), ('get', 19982)]\n",
            "NEW vocab size: 10000\n",
            "\n",
            "finished 0.00% of tweets\n",
            "finished 15.98% of tweets\n",
            "finished 31.96% of tweets\n",
            "finished 47.94% of tweets\n",
            "finished 63.92% of tweets\n",
            "finished 79.90% of tweets\n",
            "finished 95.88% of tweets\n",
            "Skipgram size: 5327787\n",
            "Most common skipgrams: [(('wear', 'mask'), 11763), (('social', 'distancing'), 6753), (('“', '”'), 6271), (('stay', 'home'), 5871), (('ca', 'n’t'), 4903), (('new', 'cases'), 3304), (('covid', '19'), 3202), (('gon', 'na'), 2875), (('‘', '’'), 2145), (('get', 'covid'), 2044)]\n",
            "\n",
            "Skipgram before: 5327787\n",
            "Skipgram after: 3528228\n",
            "NEW Most common skipgram: [(('wear', 'mask'), 11763), (('social', 'distancing'), 6753), (('“', '”'), 6271), (('stay', 'home'), 5871), (('ca', 'n’t'), 4903), (('new', 'cases'), 3304), (('covid', '19'), 3202), (('gon', 'na'), 2875), (('‘', '’'), 2145), (('get', 'covid'), 2044)]\n",
            "\n",
            "3528228 non-zero elements\n",
            "Sample PMI values\n",
            " [(('coma', 'ventilator'), 6.481049366294994),\n",
            " (('coding', 'ventilator'), 5.690738437281401),\n",
            " (('ventilator', 'tam'), 5.690738437281401),\n",
            " (('ventilator', 'occupied'), 5.690738437281401),\n",
            " (('sing', 'ventilator'), 5.654370793110527),\n",
            " (('ventilator', 'usage'), 5.610695729607865),\n",
            " (('ventilator', 'shoved'), 5.585377921623575),\n",
            " (('sweetie', 'ventilator'), 5.513057260043949),\n",
            " (('incentives', 'ventilator'), 5.424109774027452),\n",
            " (('30,000', 'ventilator'), 5.424109774027452),\n",
            " (('ventilator', 'coma'), 5.382437077626885),\n",
            " (('86', 'ventilator'), 5.3622343703093645),\n",
            " (('ventilator', 'goodbye'), 5.342431743013186),\n",
            " (('ventilator', 'manufacturers'), 5.323013657156084),\n",
            " (('overblown', 'ventilator'), 5.266924190505041),\n",
            " (('spends', 'ventilator'), 5.231206107902961),\n",
            " (('ventilator', 'speedy'), 5.17991281351541),\n",
            " (('ventilator', 'ramp'), 5.099870105841874),\n",
            " (('mommy', 'ventilator'), 5.084602633711086),\n",
            " (('fema', 'ventilator'), 5.084602633711086)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUtn7FMoUnah"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLZlAXuUVQXj"
      },
      "source": [
        "## Problem 1.2: Compute the word similarity between words using Cosine Similarity.\n",
        "\n",
        "Compute cosine similarity between the following pair of words: \n",
        "```\n",
        "('ventilator', 'covid-19')\n",
        "('ventilator', 'lockdown')\n",
        "('ventilator', 'mask')\n",
        "('ventilator', 'ppe')\n",
        "```\n",
        "\n",
        "Outputs of my code are:\n",
        "```\n",
        "('ventilator', 'covid-19') 0.17076006036635358\n",
        "('ventilator', 'lockdown') ...\n",
        "('ventilator', 'mask') 0.19229601085517933\n",
        "('ventilator', 'ppe') ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4x3CVEnsctn"
      },
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "  # write your code\n",
        "\n",
        "ventilator = build_word_vector('ventilator')\n",
        "covid19 = build_word_vector('covid-19')\n",
        "lockdown = build_word_vector('lockdown')\n",
        "mask = build_word_vector('mask')\n",
        "ppe = build_word_vector('ppe')\n",
        "\n",
        "print(('ventilator', 'covid-19'), cosine_similarity(ventilator, covid19)\n",
        "print(('ventilator', 'lockdown'), cosine_similarity(ventilator, lockdown))\n",
        "print(('ventilator', 'mask'), cosine_similarity(ventilator, mask))\n",
        "print(('ventilator', 'ppe'), cosine_similarity(ventilator, ppe))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJHApLrmLnw"
      },
      "source": [
        "## Problem 1.3: What can you tell about these words from the similarities?\n",
        "\n",
        "1. `ventilator` when compared with `covid-19, lockdown, mask, ppe`\n",
        "2. `pandemic` when compared with `covid-19, lockdown, mask, ppe`\n",
        "3. `president` compared with `trump, biden`\n",
        "4. `trudeau` compared with `trump, biden`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-FpH2k7akG"
      },
      "source": [
        "# Let's play with word2vec\n",
        "\n",
        "First let's load word2vec. I am using [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) but feel free to use any libraries or tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Jcaf1T9mHU"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = '/content/drive/My Drive/tweets/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXHjFq6upAUA"
      },
      "source": [
        "## Problem 1.4: Compute the top 5 similar words of `ventilator` using word2vec?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvff7yDBFUMg"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufUX14RV2bCY"
      },
      "source": [
        "# Problem 1.5: Word analogy\n",
        "\n",
        "If I told you the plural of `car` is `cars`, can you automatically find the plural of `hypothesis` and `man` using word2vec.\n",
        "\n",
        "Similarly, if I told you a newborn `dog` is called `puppy`, can you automatically find what are the newborn words of `cat` and `sheep` using word2vec?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCSJiYLnW1yU"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXX-DQe2oTcH"
      },
      "source": [
        "# 2. Topic Models \n",
        "\n",
        "The goal of this part is to make you familiar with topic models. You may reuse some of functions you coded for the previous assignments.\n",
        "\n",
        "## Data Download and Setup\n",
        "\n",
        "Let us start by downloading the news section of the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrStv7GPOixj"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "documents = [brown.words(fileid) for fileid in brown.fileids()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m5pYgzSqJIq"
      },
      "source": [
        "Let us inspect some of the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvgANNlUOjs"
      },
      "source": [
        "print(\"The news section of the Brown corpus contains {} documents.\".format(len(documents)))\n",
        "for i in range(3):\n",
        "  document = documents[i]\n",
        "  print(\"Document {} has {} words: {}\".format(i, len(document), document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfx34Lg4Vi9l"
      },
      "source": [
        "Finally, let us download a list of stopwords for later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlizEa3mVnWR"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "print(stopwords_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk-pkKhvWTMA"
      },
      "source": [
        "## Problem 2.1: Document-Term Matrix\n",
        "\n",
        "Create a document-term matrix with tf-idf. You should preprocess documents by: 1) lowercasing words, 2) excluding stopwords, and 3) including alphanumeric strings only.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "def create_tfidf_matrix(documents: List[List[str]]) -> (np.array, List[str]):\n",
        "  # Args:\n",
        "  #   documents: list of documents, each document being a list of words.\n",
        "  # Outputs:\n",
        "  #   tfidf_matrix: np.array of shape (num_documents, vocabulary_size)\n",
        "  #   vocabulary: a list of terms corresponding to the columns of the matrix.\n",
        "\n",
        "tfidf_matrix, vocabulary = create_tfidf_matrix(documents)\n",
        "```\n",
        "\n",
        "How sparse is this matrix? Calculate the ratio between cells with value 0 and the total number of cells. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gilMYo2eWTWD"
      },
      "source": [
        "## Problem 2.2: Latent Semantic Analysis\n",
        "\n",
        "We perform LSA to obtain document embeddings `U` and term embeddings `VT`.\n",
        "\n",
        "```python\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(tfidf_matrix, \n",
        "                              n_components=10,\n",
        "                              n_iter=100,\n",
        "                              random_state=42)\n",
        "```\n",
        "\n",
        "Define a function to find the 5 most relevant terms for each of the 10 latent dimensions (tip: you should make use of VT and the vocabulary).\n",
        "\n",
        "```python\n",
        "def extract_salient_words(VT: np.array, \n",
        "                  vocabulary: List[str]\n",
        "                  ) -> salient_words: dict[int, List[str]]:\n",
        "  # Args:\n",
        "  #  VT: a numpy array of size (n_components, vocabulary_size)\n",
        "  #  vocabulary: a list of words of size vocabulary_size\n",
        "  # Outputs:\n",
        "  #  salient_words: a dictionary with the latent dimension indices as keys and a list of its 5 most salient words as values\n",
        "\n",
        "salient_words = extract_salient_words(VT, vocabulary)\n",
        "\n",
        "for key, value in salient_words:\n",
        "  print(\"Concept {}: {}\".format(str(key), \" \".join(value)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEdR0wjZ-JN"
      },
      "source": [
        "## Problem 2.3: Document Retrieval\n",
        "\n",
        "Given a text query, view this as a mini document, and compare it to your documents in the low-dimensional space.\n",
        "\n",
        "First, we need to map a query into a pseudo-document embedding.\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ed5d0397ee6b44f72f77743029d3943932118fa2\" alt=\"Query\" height=\"35\"/>\n",
        "\n",
        "Then, you will need to implement a function to calculate the cosine similarity between this embedded query and all the document embeddings.\n",
        "\n",
        "Retrieve the indices of the top-3 documents with the highest cosine similarity with the following queries:\n",
        "\n",
        "\n",
        "```python\n",
        "query1 = ['T.', 'V.', 'Barker', 'developed', 'the', 'classification-angle', 'system']\n",
        "query2 = ['imitation', 'vs.', 'formalism' 'in', 'philosophical', 'debates']\n",
        "query3 = ['Krim', 'attended', 'the', 'University', 'of', 'North', 'Carolina', 'to', 'follow', 'Thomas', 'Wolfe']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hE7VtK5a0m5"
      },
      "source": [
        "## Problem 2.4: Document Clustering\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "num_clusters = 10\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "document_embeddings = U * Sigma\n",
        "km.fit(document_embeddings)\n",
        "clusters = km.labels_.tolist()\n",
        "print(clusters)\n",
        "```\n",
        "\n",
        "Let us now plot the document embeddings and their clusters:\n",
        "\n",
        "```python\n",
        "import umap\n",
        "embedding = umap.UMAP(n_neighbors=100, min_dist=0.5, random_state=42).fit_transform(document_embeddings)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=20, edgecolor='none')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "What are the differences you observe by using a different number of `n_components` in LSA or `n_clusters` in K-Means?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIwJM95IsLZ"
      },
      "source": [
        "## Problem 2.5 Latent Dirichlet Allocation\n",
        "\n",
        "Run LDA on `documents` using `sklearn` (find the documentation at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation))\n",
        "\n",
        "Make sure to specify `random_state=42` for replicability. \n",
        "\n",
        "What are the topics allocated to each word of document number 13? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc-j5rsLJaeX"
      },
      "source": [
        "print(documents[13])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0KIkqOqD8H"
      },
      "source": [
        "# Write your code here.\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}