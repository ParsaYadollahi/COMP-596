{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 596 - A3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "# 1. Vector Space Model of Word Meaning\n",
        "\n",
        "The goal of this problem set is to make you familiar with vector space model of word meaning. You may reuse some of functions you coded in Assignment 1. \n",
        "\n",
        "### Warning: This assignment may take substantial time to run if you are not optimizing your code. Make sure you have plenty of time to run if you are new to programming.\n",
        "\n",
        "Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n",
        "\n",
        "Caution: Since this is real language on Twitter and deals with current events, some of it could be disturbing. In the later section of the course, we will deal with ethics and what is appropriate and what is not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "*Copy function may not work. If so, manually copy the authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59aad14-6eec-4999-ac23-2eab8da7041f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see the files in the tweets folder.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98babbfd-484a-4bb1-ace8-fabbdd83ba82"
      },
      "source": [
        "!ls \"/content/drive/My Drive/tweets\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000_tweets.jsonl\n",
            "20000_tweets.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.tokenized.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.trigrams.txt\n",
            "GoogleNews-vectors-negative300.bin.gz\n",
            "stop_words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a70f740-c716-4944-a8bd-2f4199177481"
      },
      "source": [
        "# let's read tweets. These tweets are already tokenized and cleaned (Assignment 1)\n",
        "tweets = open(\"/content/drive/My Drive/tweets/covid-tweets-2020-08-10-2020-08-21.tokenized.txt\", \"r\").read().split(\"\\n\")\n",
        "tweets = [tweet.split() for tweet in tweets]\n",
        "\n",
        "print(len(tweets))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "312878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkL6n6jU1LU"
      },
      "source": [
        "## Problem 1.1: Word space model\n",
        "\n",
        "Compute the most important context words of `ventilator`. Use Pointwise Mutual Information (PMI) to rank the context words (Refer to Lecture 4).\n",
        "\n",
        "We define context as up to 3 words to the left and 3 words to the right. Ignore stop words and words that do not start with [a-z#]. Also ignore words that are not in the top 1000 frequent words.\n",
        "\n",
        "These context words define the dimensions of the vector space model. Represent each word as a vector (dictionary/counter) of context words with PMI as the importance of the context word. Print the top 20 context words for each.\n",
        "\n",
        "This is the sample output I got for `ventilator`. Your numbers need not match mine but the ranked list should look close to what I have.\n",
        "\n",
        "```\n",
        "[('put', 18.280538283196606), ('wearing', 17.587373569812726), ('even', 17.58651933524197), ('like', 17.402738298715878), ('covid', 17.172590097063086), ('patients', 16.894419647496004), ('use', 16.894298589380956), ('die', 16.89426559608771), ('days', 16.89415252713107), ('needed', 16.489137134110106), ('month', 16.48907033839664), ('weeks', 16.488913820220848), ('away', 16.48879303327717), ('week', 16.488739054051933), ('person', 16.488678720881293), ('good', 16.488160838026904), ('deaths', 16.487822204799755), ('go', 16.487564042558112), ('would', 16.48707075078768), ('one', 16.48706217686235), ('get', 16.486565870239033)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0SbP-yfiaL3"
      },
      "source": [
        "Let's first load stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFoeDtZPTy-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1643e821-1539-4329-bef6-49943672dd59"
      },
      "source": [
        "stop_words = set()\n",
        "def load_stop_words():\n",
        "  words = open(\"/content/drive/My Drive/tweets/stop_words.txt\", \"r\").read().split(\"\\n\")\n",
        "  for word in words:\n",
        "    stop_words.add(word.strip())\n",
        "\n",
        "load_stop_words()\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134\n",
            "{'been', 'too', 'are', 'at', 'that', 'itself', 'whom', 'am', 'it', 'such', 'in', 'don', 'while', 'my', 'we', 'was', 'when', 'or', 'yours', \"'ve\", 'those', 'all', 'to', 'and', 'who', 'until', 'most', 'between', 'down', 'amp', 'has', 'did', 'once', 'more', 'each', 'from', 'hers', 'ours', 'now', \"n't\", 'about', 'should', 'of', \"'ll\", 'his', 'have', 'can', 'few', 'an', '@USER', 'because', 'a', 'by', 'yourselves', \"'ld\", 'theirs', 'both', 'off', 'they', 'him', 'after', 'than', 'if', 'yourself', 'them', 'during', 'but', 'she', 'through', 'these', 'himself', 'herself', 'be', 'where', 'themselves', 'here', 'as', 'so', 'do', 'not', 'nor', 'your', 'over', 'its', 'you', 'again', 'will', 'further', 'URL', 'having', 'being', 'had', 'same', 'which', 'doing', 'only', 'her', 'with', 'why', 'what', 'how', 'very', 'their', 'no', 'just', 'some', 'before', 'he', 'any', 'against', 's', 'were', 'for', 't', 'own', 'under', 'out', 'this', 'then', 'our', 'into', 'on', 'does', 'below', 'myself', 'the', 'there', 'i', 'up', 'ourselves', 'is', 'me', 'other', 'above'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJF6PXSHQBiR"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Let's build the word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYKk9MzY2PMs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I_T8bcDUqFD",
        "outputId": "1316c286-6ff0-4db0-9144-a6142b3d5ad8"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pformat\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds, norm\n",
        "from nltk.util import skipgrams\n",
        "from string import punctuation\n",
        "from scipy import sparse\n",
        "from scipy.sparse import linalg\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import sparse\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "stemmer= PorterStemmer()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao34mmYmUtNL"
      },
      "source": [
        "unigram_counts = Counter()\n",
        "skipgram_counts = Counter()\n",
        "x2i, i2x = {}, {}\n",
        "pmi_samples = Counter()\n",
        "\n",
        "tweets_trimmed = [[lemmatizer.lemmatize(token) for token in tweet if token not in stop_words and (re.match(r\"[a-z#]+\", token[0]))] for tweet in tweets]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kd9h8A0Uywb",
        "outputId": "6e6f7501-a6ab-4fd0-b327-d7dc5a5c7776"
      },
      "source": [
        "def compute_unigrams():\n",
        "  global unigram_counts\n",
        "  for idx, tweet in enumerate(tweets_trimmed):\n",
        "    for token in tweet:\n",
        "      unigram_counts[token] += 1\n",
        "    if idx % 50000 == 0:\n",
        "        print(f'finished {idx/len(tweets_trimmed):.2%} of tweets')\n",
        "\n",
        "  # # Remove all unigrams in in top 1000\n",
        "  # most_common_unigram = unigram_counts.most_common(1000)\n",
        "  # for k,v in list(unigram_counts.copy().items()):\n",
        "  #   if (k,v) not in most_common_unigram:\n",
        "  #     del unigram_counts[k]\n",
        "  print('Most common unigram: {}'.format(unigram_counts.most_common(10)))\n",
        "  print('Trimmed vocab size should be 1000: {}'.format(len(unigram_counts)))\n",
        "\n",
        "compute_unigrams()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished 0.00% of tweets\n",
            "finished 15.98% of tweets\n",
            "finished 31.96% of tweets\n",
            "finished 47.94% of tweets\n",
            "finished 63.92% of tweets\n",
            "finished 79.90% of tweets\n",
            "finished 95.88% of tweets\n",
            "Most common unigram: [('covid', 142562), ('pandemic', 101114), ('covid-19', 67182), ('people', 64174), ('n’t', 62106), ('mask', 56814), ('get', 43362), ('like', 42086), ('coronavirus', 39898), ('trump', 39778)]\n",
            "Trimmed vocab size should be 1000: 132037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rShrYCWbU17M",
        "outputId": "3fa9c099-177d-46dd-a964-d92491e7110c"
      },
      "source": [
        "def compute_skip_grams():\n",
        "  global skipgram_counts\n",
        "  for idx, tweet in enumerate(tweets_trimmed):\n",
        "      if tweet == 'this':\n",
        "        print('------hit-----')\n",
        "      # 3 words left and right\n",
        "      skip_gram = list(skipgrams(tweet, 2, 3))\n",
        "      for context in skip_gram:\n",
        "        skipgram_counts[context] += 1\n",
        "      if idx % 50000 == 0:\n",
        "        print(f'finished {idx/len(tweets_trimmed):.2%} of tweets')\n",
        "  \n",
        "  # Remove infrequent bigrams (not in unigrams)\n",
        "  # for x, y in list(skipgram_counts.keys()):\n",
        "  #   if x not in unigram_counts or y not in unigram_counts:\n",
        "  #     del skipgram_counts[(x,y)]\n",
        "\n",
        "  print('Skipgram size: {}'.format(len(skipgram_counts)))\n",
        "  print('Most common skipgrams: {}'.format(skipgram_counts.most_common(10)))\n",
        "\n",
        "\n",
        "compute_skip_grams()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished 0.00% of tweets\n",
            "finished 15.98% of tweets\n",
            "finished 31.96% of tweets\n",
            "finished 47.94% of tweets\n",
            "finished 63.92% of tweets\n",
            "finished 79.90% of tweets\n",
            "finished 95.88% of tweets\n",
            "Skipgram size: 5711825\n",
            "Most common skipgrams: [(('wear', 'mask'), 26986), (('social', 'distancing'), 13552), (('stay', 'home'), 12088), (('ca', 'n’t'), 9980), (('new', 'case'), 8534), (('covid', 'death'), 6238), (('case', 'death'), 6208), (('wearing', 'mask'), 6062), (('gon', 'na'), 5774), (('get', 'covid'), 5204)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0dmzbJMVC60"
      },
      "source": [
        "def build_unigram_index_lookup():\n",
        "  global x2i\n",
        "  global i2x\n",
        "  for i, x in enumerate(unigram_counts.keys()):\n",
        "    x2i[x] = i\n",
        "    i2x[i] = x\n",
        "\n",
        "build_unigram_index_lookup()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ896-ClQBKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5cba64-f285-4695-abc9-3ce9287b324f"
      },
      "source": [
        "# THIS CODE IS SHITAAAAZZZZZ\n",
        "# help here: https://www.kaggle.com/gabrielaltay/word-vectors-from-pmi-matrix\n",
        "\n",
        "sxy = 0\n",
        "sx = 0\n",
        "\n",
        "def PMI(word1, word2, n):\n",
        "  # You have to store frequencies of individual words and (word, context word) \n",
        "  # pairs to compute this. You can compute them beforehand in order to avoid \n",
        "  # counting every time when this function is called.\n",
        "  # Write your code\n",
        "  global skipgram_counts\n",
        "  global unigram_counts\n",
        "  global sx\n",
        "  global sxy\n",
        "  # nxy = skipgram_counts[(word1, word2)]\n",
        "  # N = len(tweets_trimmed)\n",
        "  # nx = unigram_counts[word1]\n",
        "  # ny = unigram_counts[word2]\n",
        "  # pmi = log((nxy * N) / (nx * ny))\n",
        "  return (log((n / sxy) / (unigram_counts[word1] / sx) / (unigram_counts[word2] / sx)))\n",
        "\n",
        "\n",
        "def build_word_vector(word):\n",
        "  # Write your code\n",
        "  global skipgram_counts\n",
        "  global unigram_counts\n",
        "  global x2i\n",
        "  global i2x\n",
        "  global sx\n",
        "  global sxy\n",
        "  sx = sum(unigram_counts.values())\n",
        "  sxy = sum(skipgram_counts.values())\n",
        "  data, rows, cols = [], [], []\n",
        "  pmi_sample = Counter()\n",
        "\n",
        "  idx = 0\n",
        "  for (x, y), n in skipgram_counts.items():\n",
        "    rows.append(x2i[x])\n",
        "    cols.append(x2i[y])\n",
        "    data.append(PMI(x,y, n))\n",
        "    pmi_sample[(x,y)] = data[-1]\n",
        "    \n",
        "    if idx % 500000 == 0:\n",
        "        print(f'finished {idx/len(skipgram_counts):.2%} of tweets')\n",
        "    idx += 1\n",
        "\n",
        "\n",
        "\n",
        "  PMI_matrix = csc_matrix((data, (rows, cols)))\n",
        "  ans = Counter()\n",
        "  print(pmi_sample.most_common()[:20])\n",
        "  for (x,y), n in pmi_sample.items():\n",
        "    if (x == word or y == word):\n",
        "      ans[(x,y)] = n\n",
        "  print(ans.most_common(5))\n",
        "\n",
        "def print_top_dimensions(word_vector, n):\n",
        "  # print top n dimensions sorted in the order of importance.\n",
        "  ...\n",
        "\n",
        "v1 = build_word_vector('ventilator')\n",
        "\n",
        "# print_top_dimensions(v1, 20) # print top 20 dimensions along with their weights\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished 0.00% of tweets\n",
            "finished 8.75% of tweets\n",
            "finished 17.51% of tweets\n",
            "finished 26.26% of tweets\n",
            "finished 35.02% of tweets\n",
            "finished 43.77% of tweets\n",
            "finished 52.52% of tweets\n",
            "finished 61.28% of tweets\n",
            "finished 70.03% of tweets\n",
            "finished 78.78% of tweets\n",
            "finished 87.54% of tweets\n",
            "finished 96.29% of tweets\n",
            "[(('#julitonosecuelgue', '#regimennopodraconad'), 15.02215930132437), (('#julitonosecuelgue', '#losdanieles'), 15.02215930132437), (('#julitonosecuelgue', '#formulae'), 15.02215930132437), (('#regimennopodraconad', '#losdanieles'), 15.02215930132437), (('#regimennopodraconad', '#formulae'), 15.02215930132437), (('#losdanieles', '#formulae'), 15.02215930132437), (('coms', 'horas'), 15.02215930132437), (('#mrmojorisin', '#lionsinthestreet'), 15.02215930132437), (('#mrmojorisin', '#dawnshighway'), 15.02215930132437), (('#mrmojorisin', '#summersalmostgone'), 15.02215930132437), (('#lionsinthestreet', '#dawnshighway'), 15.02215930132437), (('#lionsinthestreet', '#summersalmostgone'), 15.02215930132437), (('#dawnshighway', '#summersalmostgone'), 15.02215930132437), (('#homegyms', '#homegymsetup'), 15.02215930132437), (('#homegyms', '#gymgirl'), 15.02215930132437), (('#homegymsetup', '#gymgirl'), 15.02215930132437), (('#homegymsetup', '#strongwomen'), 15.02215930132437), (('#gymgirl', '#strongwomen'), 15.02215930132437), (('meted', '#matthancock'), 15.02215930132437), (('#matthancock', '#stasi'), 15.02215930132437)]\n",
            "[(('ventilator', '#pitifulgovcuomo'), 9.226101550559), (('ventilator', '#dontbestupidwearamask'), 9.226101550559), (('ventilator', 'priorities&gt;&gt'), 9.226101550559), ((\"f'rs\", 'ventilator'), 9.226101550559), (('ventilator', 'tessie'), 9.226101550559)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLZlAXuUVQXj"
      },
      "source": [
        "## Problem 1.2: Compute the word similarity between words using Cosine Similarity.\n",
        "\n",
        "Compute cosine similarity between the following pair of words: \n",
        "```\n",
        "('ventilator', 'covid-19')\n",
        "('ventilator', 'lockdown')\n",
        "('ventilator', 'mask')\n",
        "('ventilator', 'ppe')\n",
        "```\n",
        "\n",
        "Outputs of my code are:\n",
        "```\n",
        "('ventilator', 'covid-19') 0.17076006036635358\n",
        "('ventilator', 'lockdown') ...\n",
        "('ventilator', 'mask') 0.19229601085517933\n",
        "('ventilator', 'ppe') ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4x3CVEnsctn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "675041ef-9634-41af-8a71-0e8da9c077bd"
      },
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "  v2 = list(vector1.values())\n",
        "  v2 = list(vector2.values())\n",
        "\n",
        "  dot_prod = np.dot(v1, v2)\n",
        "  mag = LA.norm(v1) * LA.norm(v2)\n",
        "  return dot_prod / mag\n",
        "\n",
        "ventilator = build_word_vector('ventilator')\n",
        "covid19 = build_word_vector('covid-19')\n",
        "lockdown = build_word_vector('lockdown')\n",
        "mask = build_word_vector('mask')\n",
        "ppe = build_word_vector('ppe')\n",
        "\n",
        "print(('ventilator', 'covid-19'), cosine_similarity(ventilator, covid19))\n",
        "print(('ventilator', 'lockdown'), cosine_similarity(ventilator, lockdown))\n",
        "print(('ventilator', 'mask'), cosine_similarity(ventilator, mask))\n",
        "print(('ventilator', 'ppe'), cosine_similarity(ventilator, ppe))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished 0.00% of tweets\n",
            "finished 9.81% of tweets\n",
            "finished 19.63% of tweets\n",
            "finished 29.44% of tweets\n",
            "finished 39.26% of tweets\n",
            "finished 49.07% of tweets\n",
            "finished 58.88% of tweets\n",
            "finished 68.70% of tweets\n",
            "finished 78.51% of tweets\n",
            "finished 88.32% of tweets\n",
            "finished 98.14% of tweets\n",
            "[(('509,539', '8,583'), 14.61238800859867), (('8,583', '1,754'), 14.61238800859867), (('#julitonosecuelgu', '#regimennopodraconad'), 14.61238800859867), (('#julitonosecuelgu', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#formula'), 14.61238800859867), (('#losdaniel', '#formula'), 14.61238800859867), (('3217', '2544'), 14.61238800859867), (('#mrmojorisin', '#lionsinthestreet'), 14.61238800859867), (('#mrmojorisin', '#dawnshighway'), 14.61238800859867), (('#mrmojorisin', '#summersalmostgon'), 14.61238800859867), (('#lionsinthestreet', '#dawnshighway'), 14.61238800859867), (('#lionsinthestreet', '#summersalmostgon'), 14.61238800859867), (('#dawnshighway', '#summersalmostgon'), 14.61238800859867), (('#homegymsetup', '#gymgirl'), 14.61238800859867), (('#homegymsetup', '#strongwomen'), 14.61238800859867), (('#gymgirl', '#strongwomen'), 14.61238800859867), (('13,619,967', '68.07'), 14.61238800859867), (('634929', '1534278'), 14.61238800859867), (('634929', '44466'), 14.61238800859867)]\n",
            "[]\n",
            "finished 0.00% of tweets\n",
            "finished 9.81% of tweets\n",
            "finished 19.63% of tweets\n",
            "finished 29.44% of tweets\n",
            "finished 39.26% of tweets\n",
            "finished 49.07% of tweets\n",
            "finished 58.88% of tweets\n",
            "finished 68.70% of tweets\n",
            "finished 78.51% of tweets\n",
            "finished 88.32% of tweets\n",
            "finished 98.14% of tweets\n",
            "[(('509,539', '8,583'), 14.61238800859867), (('8,583', '1,754'), 14.61238800859867), (('#julitonosecuelgu', '#regimennopodraconad'), 14.61238800859867), (('#julitonosecuelgu', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#formula'), 14.61238800859867), (('#losdaniel', '#formula'), 14.61238800859867), (('3217', '2544'), 14.61238800859867), (('#mrmojorisin', '#lionsinthestreet'), 14.61238800859867), (('#mrmojorisin', '#dawnshighway'), 14.61238800859867), (('#mrmojorisin', '#summersalmostgon'), 14.61238800859867), (('#lionsinthestreet', '#dawnshighway'), 14.61238800859867), (('#lionsinthestreet', '#summersalmostgon'), 14.61238800859867), (('#dawnshighway', '#summersalmostgon'), 14.61238800859867), (('#homegymsetup', '#gymgirl'), 14.61238800859867), (('#homegymsetup', '#strongwomen'), 14.61238800859867), (('#gymgirl', '#strongwomen'), 14.61238800859867), (('13,619,967', '68.07'), 14.61238800859867), (('634929', '1534278'), 14.61238800859867), (('634929', '44466'), 14.61238800859867)]\n",
            "[(('#gocardsn', 'covid-19'), 4.883432430581666), (('covid-19', '#anu'), 4.190285250021721), (('covid-19', 'site.\\u200b'), 4.190285250021721), (('covid-19', 'wallop'), 4.190285250021721), (('u.s.,china', 'covid-19'), 4.190285250021721)]\n",
            "finished 0.00% of tweets\n",
            "finished 9.81% of tweets\n",
            "finished 19.63% of tweets\n",
            "finished 29.44% of tweets\n",
            "finished 39.26% of tweets\n",
            "finished 49.07% of tweets\n",
            "finished 58.88% of tweets\n",
            "finished 68.70% of tweets\n",
            "finished 78.51% of tweets\n",
            "finished 88.32% of tweets\n",
            "finished 98.14% of tweets\n",
            "[(('509,539', '8,583'), 14.61238800859867), (('8,583', '1,754'), 14.61238800859867), (('#julitonosecuelgu', '#regimennopodraconad'), 14.61238800859867), (('#julitonosecuelgu', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#formula'), 14.61238800859867), (('#losdaniel', '#formula'), 14.61238800859867), (('3217', '2544'), 14.61238800859867), (('#mrmojorisin', '#lionsinthestreet'), 14.61238800859867), (('#mrmojorisin', '#dawnshighway'), 14.61238800859867), (('#mrmojorisin', '#summersalmostgon'), 14.61238800859867), (('#lionsinthestreet', '#dawnshighway'), 14.61238800859867), (('#lionsinthestreet', '#summersalmostgon'), 14.61238800859867), (('#dawnshighway', '#summersalmostgon'), 14.61238800859867), (('#homegymsetup', '#gymgirl'), 14.61238800859867), (('#homegymsetup', '#strongwomen'), 14.61238800859867), (('#gymgirl', '#strongwomen'), 14.61238800859867), (('13,619,967', '68.07'), 14.61238800859867), (('634929', '1534278'), 14.61238800859867), (('634929', '44466'), 14.61238800859867)]\n",
            "[(('lockdown', 'everythibg'), 5.92216630730007), (('bokuaka', 'lockdown'), 5.92216630730007), (('lockdown', 'ule'), 5.92216630730007), (('lockdown', 'babyy'), 5.92216630730007), (('jisoo', 'lockdown'), 5.2830863480104)]\n",
            "finished 0.00% of tweets\n",
            "finished 9.81% of tweets\n",
            "finished 19.63% of tweets\n",
            "finished 29.44% of tweets\n",
            "finished 39.26% of tweets\n",
            "finished 49.07% of tweets\n",
            "finished 58.88% of tweets\n",
            "finished 68.70% of tweets\n",
            "finished 78.51% of tweets\n",
            "finished 88.32% of tweets\n",
            "finished 98.14% of tweets\n",
            "[(('509,539', '8,583'), 14.61238800859867), (('8,583', '1,754'), 14.61238800859867), (('#julitonosecuelgu', '#regimennopodraconad'), 14.61238800859867), (('#julitonosecuelgu', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#formula'), 14.61238800859867), (('#losdaniel', '#formula'), 14.61238800859867), (('3217', '2544'), 14.61238800859867), (('#mrmojorisin', '#lionsinthestreet'), 14.61238800859867), (('#mrmojorisin', '#dawnshighway'), 14.61238800859867), (('#mrmojorisin', '#summersalmostgon'), 14.61238800859867), (('#lionsinthestreet', '#dawnshighway'), 14.61238800859867), (('#lionsinthestreet', '#summersalmostgon'), 14.61238800859867), (('#dawnshighway', '#summersalmostgon'), 14.61238800859867), (('#homegymsetup', '#gymgirl'), 14.61238800859867), (('#homegymsetup', '#strongwomen'), 14.61238800859867), (('#gymgirl', '#strongwomen'), 14.61238800859867), (('13,619,967', '68.07'), 14.61238800859867), (('634929', '1534278'), 14.61238800859867), (('634929', '44466'), 14.61238800859867)]\n",
            "[(('mask', 'transmission-'), 5.040499585580779), (('maskgina', 'mask'), 5.040499585580779), (('mask', 'loveyouby'), 5.040499585580779), (('mask', '#bcleg'), 4.347352405020834), (('mask', 'problem?mor'), 4.347352405020834)]\n",
            "finished 0.00% of tweets\n",
            "finished 9.81% of tweets\n",
            "finished 19.63% of tweets\n",
            "finished 29.44% of tweets\n",
            "finished 39.26% of tweets\n",
            "finished 49.07% of tweets\n",
            "finished 58.88% of tweets\n",
            "finished 68.70% of tweets\n",
            "finished 78.51% of tweets\n",
            "finished 88.32% of tweets\n",
            "finished 98.14% of tweets\n",
            "[(('509,539', '8,583'), 14.61238800859867), (('8,583', '1,754'), 14.61238800859867), (('#julitonosecuelgu', '#regimennopodraconad'), 14.61238800859867), (('#julitonosecuelgu', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#losdaniel'), 14.61238800859867), (('#regimennopodraconad', '#formula'), 14.61238800859867), (('#losdaniel', '#formula'), 14.61238800859867), (('3217', '2544'), 14.61238800859867), (('#mrmojorisin', '#lionsinthestreet'), 14.61238800859867), (('#mrmojorisin', '#dawnshighway'), 14.61238800859867), (('#mrmojorisin', '#summersalmostgon'), 14.61238800859867), (('#lionsinthestreet', '#dawnshighway'), 14.61238800859867), (('#lionsinthestreet', '#summersalmostgon'), 14.61238800859867), (('#dawnshighway', '#summersalmostgon'), 14.61238800859867), (('#homegymsetup', '#gymgirl'), 14.61238800859867), (('#homegymsetup', '#strongwomen'), 14.61238800859867), (('#gymgirl', '#strongwomen'), 14.61238800859867), (('13,619,967', '68.07'), 14.61238800859867), (('634929', '1534278'), 14.61238800859867), (('634929', '44466'), 14.61238800859867)]\n",
            "[(('hoax?h', 'ppe'), 8.154049725253879), (('.@zarahsultana', 'ppe'), 8.154049725253879), (('ppe', '4frontlin'), 8.154049725253879), (('rs.3000/-', 'ppe'), 8.154049725253879), (('ppe', 'henessi'), 8.154049725253879)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-f8247eb58a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mppe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ppe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ventilator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'covid-19'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mventilator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovid19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ventilator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lockdown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mventilator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlockdown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ventilator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mventilator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-f8247eb58a41>\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(vector1, vector2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdot_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'values'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJHApLrmLnw"
      },
      "source": [
        "## Problem 1.3: What can you tell about these words from the similarities?\n",
        "\n",
        "1. `ventilator` when compared with `covid-19, lockdown, mask, ppe`\n",
        "2. `pandemic` when compared with `covid-19, lockdown, mask, ppe`\n",
        "3. `president` compared with `trump, biden`\n",
        "4. `trudeau` compared with `trump, biden`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-FpH2k7akG"
      },
      "source": [
        "# Let's play with word2vec\n",
        "\n",
        "First let's load word2vec. I am using [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) but feel free to use any libraries or tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnAsRw6CiIY6"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Jcaf1T9mHU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "871060c7-1992-42c3-bdc1-0da0dcc4904e"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "EMBEDDING_FILE = '/content/drive/My Drive/tweets/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-61c302034be3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/tweets/GoogleNews-vectors-negative300.bin.gz'\u001b[0m \u001b[0;31m# from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mdecompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/local_file.py\u001b[0m in \u001b[0;36mopen_uri\u001b[0;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mparsed_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uri_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/tweets/GoogleNews-vectors-negative300.bin.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXHjFq6upAUA"
      },
      "source": [
        "## Problem 1.4: Compute the top 5 similar words of `ventilator` using word2vec?\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvff7yDBFUMg"
      },
      "source": [
        "# Write your code here\n",
        "word2vec.most_similar(['ventilator'] , topn = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufUX14RV2bCY"
      },
      "source": [
        "# Problem 1.5: Word analogy\n",
        "\n",
        "If I told you the plural of `car` is `cars`, can you automatically find the plural of `hypothesis` and `man` using word2vec.\n",
        "\n",
        "Similarly, if I told you a newborn `dog` is called `puppy`, can you automatically find what are the newborn words of `cat` and `sheep` using word2vec?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWJjb0Vh61JN"
      },
      "source": [
        "word2vec.most_similar(positive=['hypothesis','cars'], negative=['car'] )[0][0]\n",
        "word2vec.most_similar(positive=['man','cars'], negative=['car'] )[0][0]\n",
        "\n",
        "word2vec.most_similar(positive=['cat','puppy'], negative=['dog'] )[0][0]\n",
        "word2vec.most_similar(positive=['sheep','puppy'], negative=['dog'] )[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXX-DQe2oTcH"
      },
      "source": [
        "# 2. Topic Models \n",
        "\n",
        "The goal of this part is to make you familiar with topic models. You may reuse some of functions you coded for the previous assignments.\n",
        "\n",
        "## Data Download and Setup\n",
        "\n",
        "Let us start by downloading the news section of the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrStv7GPOixj"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "documents = [brown.words(fileid) for fileid in brown.fileids()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m5pYgzSqJIq"
      },
      "source": [
        "Let us inspect some of the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvgANNlUOjs"
      },
      "source": [
        "print(\"The news section of the Brown corpus contains {} documents.\".format(len(documents)))\n",
        "for i in range(3):\n",
        "  document = documents[i]\n",
        "  print(\"Document {} has {} words: {}\".format(i, len(document), document))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfx34Lg4Vi9l"
      },
      "source": [
        "Finally, let us download a list of stopwords for later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlizEa3mVnWR"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "print(stopwords_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk-pkKhvWTMA"
      },
      "source": [
        "## Problem 2.1: Document-Term Matrix\n",
        "\n",
        "Create a document-term matrix with tf-idf. You should preprocess documents by: 1) lowercasing words, 2) excluding stopwords, and 3) including alphanumeric strings only.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "def create_tfidf_matrix(documents: List[List[str]]) -> (np.array, List[str]):\n",
        "  # Args:\n",
        "  #   documents: list of documents, each document being a list of words.\n",
        "  # Outputs:\n",
        "  #   tfidf_matrix: np.array of shape (num_documents, vocabulary_size)\n",
        "  #   vocabulary: a list of terms corresponding to the columns of the matrix.\n",
        "\n",
        "tfidf_matrix, vocabulary = create_tfidf_matrix(documents)\n",
        "```\n",
        "\n",
        "How sparse is this matrix? Calculate the ratio between cells with value 0 and the total number of cells. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmQcVe96vzQx"
      },
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "def preprocess(documents: List[List[str]]):\n",
        "  return_docs = []\n",
        "  documents_processed = [[token.lower() for token in document if token not in stop_words and token.isalnum()] for document in documents]\n",
        "  for doc in documents_processed:\n",
        "    return_docs.append(' '.join(doc))\n",
        "\n",
        "  return return_docs\n",
        "\n",
        "documents_processed = preprocess(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqASR-NvGDJ"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def create_tfidf_matrix(documents: List[List[str]]) -> (np.array, List[str]):\n",
        "  # Args:\n",
        "  #   documents: list of documents, each document being a list of words.\n",
        "  # Outputs:\n",
        "  #   tfidf_matrix: np.array of shape (num_documents, vocabulary_size)\n",
        "  #   vocabulary: a list of terms corresponding to the columns of the matrix.\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  doc_vec = vectorizer.fit_transform(documents)\n",
        "  tfidf_df = pd.DataFrame(doc_vec.toarray(), columns = vectorizer.get_feature_names())\n",
        "  tfidf_matrix = tfidf_df.to_numpy()\n",
        "  voc = list(tfidf_df.columns.values)\n",
        "  return (tfidf_matrix, voc)\n",
        "\n",
        "tfidf_matrix, vocabulary = create_tfidf_matrix(documents_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWS-t34chskT"
      },
      "source": [
        "# SPACITY OF MATRIX\n",
        "from numpy import count_nonzero\n",
        "\n",
        "sparsity = 1.0 - ( count_nonzero(tfidf_matrix) / float(tfidf_matrix.size) )\n",
        "print(\"Sparcity of a matrix is {}\".format(sparsity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gilMYo2eWTWD"
      },
      "source": [
        "## Problem 2.2: Latent Semantic Analysis\n",
        "\n",
        "We perform LSA to obtain document embeddings `U` and term embeddings `VT`.\n",
        "\n",
        "```python\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(tfidf_matrix, \n",
        "                              n_components=10,\n",
        "                              n_iter=100,\n",
        "                              random_state=42)\n",
        "```\n",
        "\n",
        "Define a function to find the 5 most relevant terms for each of the 10 latent dimensions (tip: you should make use of VT and the vocabulary).\n",
        "\n",
        "```python\n",
        "def extract_salient_words(VT: np.array, \n",
        "                  vocabulary: List[str]\n",
        "                  ) -> salient_words: dict[int, List[str]]:\n",
        "  # Args:\n",
        "  #  VT: a numpy array of size (n_components, vocabulary_size)\n",
        "  #  vocabulary: a list of words of size vocabulary_size\n",
        "  # Outputs:\n",
        "  #  salient_words: a dictionary with the latent dimension indices as keys and a list of its 5 most salient words as values\n",
        "\n",
        "salient_words = extract_salient_words(VT, vocabulary)\n",
        "\n",
        "for key, value in salient_words:\n",
        "  print(\"Concept {}: {}\".format(str(key), \" \".join(value)))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMe5SFAtkGRn"
      },
      "source": [
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "U, Sigma, VT = randomized_svd(tfidf_matrix, \n",
        "                              n_components=10,\n",
        "                              n_iter=100,\n",
        "                              random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Amv_EdXkH5y"
      },
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def extract_salient_words(VT: np.array, vocabulary: List[str]) -> Dict[int, List[str]]:\n",
        "  # Args:\n",
        "  #  VT: a numpy array of size (n_components, vocabulary_size)\n",
        "  #  vocabulary: a list of words of size vocabulary_size\n",
        "  # Outputs:\n",
        "  #  salient_words: a dictionary with the latent dimension indices as keys and a list of its 5 most salient words as values\n",
        "  salient_words: Dict[int, List[str]] = {}\n",
        "  for i in range(len(VT)):\n",
        "    tfidf_sorting = np.argsort(VT[i]).flatten()[::-1]\n",
        "    n = 5\n",
        "    voc = np.array(vocabulary)\n",
        "    top_n = voc[tfidf_sorting]\n",
        "    salient_words[i] = top_n[:n].tolist()\n",
        "  return salient_words\n",
        "\n",
        "salient_words = extract_salient_words(VT, vocabulary)\n",
        "\n",
        "for key, value in salient_words.items():\n",
        "  print(\"Concept {}: {}\".format(str(key), \" \".join(value)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEdR0wjZ-JN"
      },
      "source": [
        "## Problem 2.3: Document Retrieval\n",
        "\n",
        "Given a text query, view this as a mini document, and compare it to your documents in the low-dimensional space.\n",
        "\n",
        "First, we need to map a query into a pseudo-document embedding.\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ed5d0397ee6b44f72f77743029d3943932118fa2\" alt=\"Query\" height=\"35\"/>\n",
        "\n",
        "Then, you will need to implement a function to calculate the cosine similarity between this embedded query and all the document embeddings.\n",
        "\n",
        "Retrieve the indices of the top-3 documents with the highest cosine similarity with the following queries:\n",
        "\n",
        "\n",
        "```python\n",
        "query1 = ['T.', 'V.', 'Barker', 'developed', 'the', 'classification-angle', 'system']\n",
        "query2 = ['imitation', 'vs.', 'formalism' 'in', 'philosophical', 'debates']\n",
        "query3 = ['Krim', 'attended', 'the', 'University', 'of', 'North', 'Carolina', 'to', 'follow', 'Thomas', 'Wolfe']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hE7VtK5a0m5"
      },
      "source": [
        "## Problem 2.4: Document Clustering\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "num_clusters = 10\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "document_embeddings = U * Sigma\n",
        "km.fit(document_embeddings)\n",
        "clusters = km.labels_.tolist()\n",
        "print(clusters)\n",
        "```\n",
        "\n",
        "Let us now plot the document embeddings and their clusters:\n",
        "\n",
        "```python\n",
        "import umap\n",
        "embedding = umap.UMAP(n_neighbors=100, min_dist=0.5, random_state=42).fit_transform(document_embeddings)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=20, edgecolor='none')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "What are the differences you observe by using a different number of `n_components` in LSA or `n_clusters` in K-Means?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iok1VFGS8Q2V"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN2a4xSJAP8_"
      },
      "source": [
        "num_clusters = 20\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "document_embeddings = U * Sigma\n",
        "km.fit(document_embeddings)\n",
        "clusters = km.labels_.tolist()\n",
        "print(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4NA6pa38SQB"
      },
      "source": [
        "embedding = umap.UMAP(n_neighbors=100, min_dist=0.5, random_state=42).fit_transform(document_embeddings)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, s=20, edgecolor='none')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIwJM95IsLZ"
      },
      "source": [
        "## Problem 2.5 Latent Dirichlet Allocation\n",
        "\n",
        "Run LDA on `documents` using `sklearn` (find the documentation at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation))\n",
        "\n",
        "Make sure to specify `random_state=42` for replicability. \n",
        "\n",
        "What are the topics allocated to each word of document number 13? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc-j5rsLJaeX"
      },
      "source": [
        "print(documents[13])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0KIkqOqD8H"
      },
      "source": [
        "# Write your code here.\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}